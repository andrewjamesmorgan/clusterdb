<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>multi-host &#8211; Andrew Morgan on Databases</title>
	<atom:link href="./feed/index.html" rel="self" type="application/rss+xml" />
	<link>./../../index.html</link>
	<description>Database technologies - especially around scalability and High Availability</description>
	<lastBuildDate>Fri, 10 Jul 2009 19:33:55 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	
	<item>
		<title>Deploying MySQL Cluster over multiple hosts</title>
		<link>./../../mysql-cluster/deploying-mysql-cluster-over-multiple-hosts/index.html</link>
					<comments>./../../mysql-cluster/deploying-mysql-cluster-over-multiple-hosts/index.html#comments</comments>
		
		<dc:creator><![CDATA[andrew]]></dc:creator>
		<pubDate>Fri, 10 Jul 2009 19:33:55 +0000</pubDate>
				<category><![CDATA[MySQL Cluster]]></category>
		<category><![CDATA[LINUX]]></category>
		<category><![CDATA[multi-host]]></category>
		<category><![CDATA[MySQL]]></category>
		<guid isPermaLink="false">./../../index.html?p=262</guid>

					<description><![CDATA[This post builds upon the earlier article (Creating a simple Cluster on a single LINUX host) which explained how to install and run a Cluster where all of the nodes run on the same physical host. The single host solution is not great for a real deployment &#8211; MySQL Cluster is designed to provide a]]></description>
										<content:encoded><![CDATA[<p>This post builds upon the earlier article (<a href="./../../mysql-cluster/creating-a-simple-cluster-on-a-single-linux-host/index.html" target="_blank">Creating a simple Cluster on a single LINUX host</a>) which explained how to install and run a Cluster where all of the nodes run on the same physical host.</p>
<p>The single host solution is not great for a real deployment &#8211; MySQL Cluster is designed to provide a High Availability (HA) solution by synchronously replicating data between data nodes &#8211; if all of the data nodes run on a single host that that machine is a single point of failure.</p>
<div id="attachment_264" style="width: 310px" class="wp-caption alignright"><a href="./../../wp-content/uploads/2009/07/multi_host_cluster.jpg"><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-264" class="size-medium wp-image-264" title="Multi-Host Cluster" src="./../../wp-content/uploads/2009/07/multi_host_cluster-300x184.jpg" alt="MySQL Cluster running accross 2 hosts" width="300" height="184" srcset="./../../wp-content/uploads/2009/07/multi_host_cluster-300x184.jpg 300w, ./../../wp-content/uploads/2009/07/multi_host_cluster.jpg 753w" sizes="(max-width: 300px) 100vw, 300px" /></a><p id="caption-attachment-264" class="wp-caption-text">MySQL Cluster running accross 2 hosts</p></div>
<p>This article demonstrates how to split the nodes between hosts; the configuration will still be fairly simple, using just 2 machines but it should be obvious how to extend it to more.</p>
<p>This new Cluster will be split across 2 machines, with all functions duplicated on each machine as shown in the diagram.</p>
<h3>Downloading and installing</h3>
<p>In the following example, host &#8220;ws1&#8221; has the IP Address 192.168.0.3 and &#8220;ws2&#8221; has 192.168.0.4.</p>
<p>Browse to the <a href="http://dev.mysql.com/downloads/cluster/7.0.html#Linux_(non_RPM_packages)" target="_blank">MySQL Cluster LINUX download page</a> at mysql.com and download the correct version (32 or 64 bit) and store it in the desired directory (in my case, /home/billy/mysql) <strong>on both hosts</strong> and then extract and rename the new folder to something easier to work with&#8230;</p>
<pre style="padding-left: 30px; font-size: 11px"><span style="color: #993300;">[billy@ws1 mysql]$ tar xvf mysql-cluster-gpl-7.0.6-linux-x86_64-glibc23.tar.gz
[billy@ws1 mysql]$ mv mysql-cluster-gpl-7.0.6-linux-x86_64-glibc23 7_0_6

</span><span style="color: #993300;"><span style="color: #993366;">[billy@ws2 mysql]$ tar xvf mysql-cluster-gpl-7.0.6-linux-x86_64-glibc23.tar.gz
[billy@ws2 mysql]$ mv mysql-cluster-gpl-7.0.6-linux-x86_64-glibc23 7_0_6</span>
</span></pre>
<p>Setup the files that will be needed for the MySQL Server processes (mysqld)  to run correctly <strong>on each host</strong>&#8230;</p>
<pre style="padding-left: 30px; font-size: 11px"><span style="color: #993300;">[</span><span style="color: #993300;"><span style="color: #993300;">billy@ws1 mysql]$ cd 7_0_6
</span>[billy@ws1 7_0_6]$ scripts/mysql_install_db --basedir=/home/billy/mysql/7_0_6 --datadir=/home/billy/mysql/7_0_6/data

</span><span style="color: #993366;">[billy@ws2 mysql]$ cd 7_0_6
[billy@ws2 7_0_6]$ scripts/mysql_install_db --basedir=/home/billy/mysql/7_0_6 --datadir=/home/billy/mysql/7_0_6/data</span></pre>
<h3>Configure and run the Cluster</h3>
<p>Create a sub-directory called &#8220;conf&#8221; <strong>on each host</strong> and create the following 3 files:</p>
<p style="padding-left: 30px;"><strong>config.ini &#8211; </strong><span style="text-decoration: underline;">note that this file is used by the management nodes and as we&#8217;re going to run an ndb_mgmd process on each host, this is created on both ws1 and ws2</span> <strong><br />
</strong></p>
<pre style="padding-left: 60px; font-size: 11px"><span style="color: #333399;">[ndbd default]
noofreplicas=2

[ndbd]
hostname=192.168.0.3
id=1

[ndbd]
hostname=192.168.0.4
id=2

[ndb_mgmd]
id = 101
hostname=192.168.0.3

</span><span style="color: #333399;">[ndb_mgmd]
id = 102
hostname=192.168.0.4</span>
<span style="color: #333399;">
[mysqld]
id=51
hostname=192.168.0.3

[mysqld]
id=52
hostname=192.168.0.4
</span></pre>
<p style="padding-left: 30px;"><strong>my.1.conf</strong> &#8211; <span style="text-decoration: underline;">to be used by the MySQL Server on ws1 and so store on ws1</span></p>
<pre style="padding-left: 60px; font-size: 11px"><span style="color: #333399;">[mysqld]
ndb-nodeid=51
ndbcluster
datadir=/home/billy/mysql/7_0_6/data
basedir=/home/billy/mysql/7_0_6
port=3306
server-id=51
log-bin</span></pre>
<p style="padding-left: 30px;"><strong>my.2.conf </strong>&#8211; <span style="text-decoration: underline;">to be used by the MySQL Server on ws2 and so store on ws2<br />
</span></p>
<pre style="padding-left: 60px; font-size: 11px"><span style="color: #333399;">[mysqld]
ndb-nodeid=52
ndbcluster
datadir=/home/billy/mysql/7_0_6/data
basedir=/home/billy/mysql/7_0_6
port=3306
server-id=52
log-bin</span></pre>
<p>Those files configure the nodes that make up the Cluster.  From a command prompt window, launch the management nodes:</p>
<pre style="padding-left: 30px; font-size: 11px"><span style="color: #993300;">[billy@ws1 7_0_6]$ bin/ndb_mgmd --initial -f conf/config.ini --configdir=</span><span style="color: #993300;">/home/billy/mysql/7_0_6</span><span style="color: #993300;">/conf

</span><span style="color: #993366;">[billy@ws2 7_0_6]$ bin/ndb_mgmd --initial -f conf/config.ini --configdir=/home/billy/mysql/7_0_6/conf
</span></pre>
<p>Check that the management nodes are up and running:</p>
<pre style="padding-left: 30px; font-size: 11px"><span style="color: #993300;">[billy@ws1 7_0_6]$ bin/ndb_mgm
ndb_mgm&gt; show
<span style="color: #ff0000;"><span style="color: #993300;">Connected to Management Server at: localhost:1186
Cluster Configuration
---------------------
[ndbd(NDB)]	2 node(s)
id=1 (not connected, accepting connect from 192.168.0.3)
id=2 (not connected, accepting connect from 192.168.0.4)

[ndb_mgmd(MGM)]	2 node(s)
id=101	@192.168.0.3  (mysql-5.1.34 ndb-7.0.6)
id=102 (not connected, accepting connect from 192.168.0.4)

[mysqld(API)]	2 node(s)
id=51 (not connected, accepting connect from 192.168.0.3)
id=52 (not connected, accepting connect from 192.168.0.4</span>)
</span>
ndb_mgm&gt; quit

</span><span style="color: #993300;">[billy@ws1 7_0_6]$ bin/ndb_mgm -c 192.168.0.4:1186
<span style="color: #993300;">ndb_mgm&gt; show</span></span><span style="color: #993300;">
Connected to Management Server at: 192.168.0.4:1186
Cluster Configuration
---------------------
[ndbd(NDB)]	2 node(s)
id=1 (not connected, accepting connect from 192.168.0.3)
id=2 (not connected, accepting connect from 192.168.0.4)

[ndb_mgmd(MGM)]	2 node(s)
id=101 (not connected, accepting connect from 192.168.0.3)
id=102	@192.168.0.4  (mysql-5.1.34 ndb-7.0.6)

[mysqld(API)]	2 node(s)
id=51 (not connected, accepting connect from 192.168.0.3)
id=52 (not connected, accepting connect from 192.168.0.4)</span>
<span style="color: #993300;">ndb_mgm&gt; quit</span></pre>
<p>and then start the 2 data nodes (ndbd) and 2 MySQL API/Server nodes (mysqld) and then check that they&#8217;re all up and running:</p>
<pre style="padding-left: 30px; font-size: 11px"><span style="color: #993300;">[billy@ws1 7_0_6]$ bin/ndbd --initial -c localhost:1186
2009-06-17 13:05:47 [ndbd] INFO     -- Configuration fetched from 'localhost:1186', generation: 1

<span style="color: #993366;">[billy@ws2 7_0_6]$ bin/ndbd --initial -c localhost:1186
2009-06-17 13:05:51 [ndbd] INFO     -- Configuration fetched from 'localhost:1186', generation: 1</span>

[billy@ws1 7_0_6]$ bin/mysqld --defaults-file=conf/my.1.conf&amp;

<span style="color: #993366;">[billy@ws2 7_0_6]$ bin/mysqld --defaults-file=conf/my.2.conf&amp;</span>

[billy@ws1 7_0_6]$ bin/ndb_mgm
-- NDB Cluster -- Management Client --
ndb_mgm&gt; show
<span style="color: #993300;">Connected to Management Server at: localhost:1186
Cluster Configuration
---------------------
[ndbd(NDB)]	2 node(s)
id=1	@127.0.0.1  (mysql-5.1.34 ndb-7.0.6, Nodegroup: 0, Master)
id=2	@192.168.0.4  (mysql-5.1.34 ndb-7.0.6, Nodegroup: 0)

[ndb_mgmd(MGM)]	2 node(s)
id=101	@127.0.0.1  (mysql-5.1.34 ndb-7.0.6)
id=102	@127.0.0.1  (mysql-5.1.34 ndb-7.0.6)

[mysqld(API)]	2 node(s)
id=51	@192.168.0.3  (mysql-5.1.34 ndb-7.0.6)
id=52	@192.168.0.4  (mysql-5.1.34 ndb-7.0.6)</span>
ndb_mgm&gt; quit</span></pre>
<h3>Using the Cluster</h3>
<p>There are now 2 API nodes/MySQL Servers/mysqlds running on the 2 different hosts; both accessing the same data. Each of those nodes can be accessed by the mysql client using the hostname and ports that were configured in the my.X.cnf files. For example, we can access the first of those nodes (node 51) in the following way (each API node is accessed using the host and port number in its associate my.X.cnf file:</p>
<pre style="padding-left: 30px; font-size: 11px"><span style="color: #993300;">[billy@ws1 7_0_6]$ bin/mysql -h localhost -P 3306
Welcome to the MySQL monitor.  Commands end with ; or g.
Your MySQL connection id is 4
Server version: 5.1.34-ndb-7.0.6-cluster-gpl-log MySQL Cluster Server (GPL)
Type 'help;' or 'h' for help. Type 'c' to clear the current input statement.
mysql&gt; use test;
Database changed
mysql&gt; create table assets (name varchar(30) not null primary key,
-&gt; value int) engine=ndb;
Query OK, 0 rows affected (0.99 sec)
mysql&gt; insert into assets values ('Car','1900');
Query OK, 1 row affected (0.03 sec)
mysql&gt; select * from assets;
+------+-------+
| name | value |
+------+-------+
| Car  |  1900 |
+------+-------+
1 row in set (0.00 sec)
mysql&gt; quit
Bye</span></pre>
<p>If you see &#8220;<span style="color: #ff0000;">ERROR 1130 (HY000): Host &#8216;192.168.0.3&#8217; is not allowed to connect to this MySQL server</span>&#8221; then you need to set up the privileges (repeat on each host) &#8211; in a real system, you&#8217;d likely want tighter security that this:</p>
<pre style="padding-left: 30px; font-size: 11px"><span style="color: #993300;">[billy@ws1 7_0_6]$ bin/mysql -u root
Welcome to the MySQL monitor.  Commands end with ; or g.
Your MySQL connection id is 3
Server version: 5.1.34-ndb-7.0.6-cluster-gpl-log MySQL Cluster Server (GPL)
Type 'help;' or 'h' for help. Type 'c' to clear the current input statement.
mysql&gt; GRANT ALL ON *.* TO ''@'localhost';
Query OK, 0 rows affected (0.00 sec)
mysql&gt; GRANT ALL ON *.* TO ''@'192.168.0.4';
Query OK, 0 rows affected (0.00 sec)
mysql&gt; quit
Bye</span>

<span style="color: #993366;">[billy@ws2 7_0_6]$ bin/mysql -u root
Welcome to the MySQL monitor.  Commands end with ; or g.
Your MySQL connection id is 8
Server version: 5.1.34-ndb-7.0.6-cluster-gpl-log MySQL Cluster Server (GPL)
Type 'help;' or 'h' for help. Type 'c' to clear the current input statement.
mysql&gt; GRANT ALL ON *.* TO ''@'localhost';
Query OK, 0 rows affected (0.01 sec)
mysql&gt; GRANT ALL ON *.* TO ''@'192.168.0.3';
Query OK, 0 rows affected (0.00 sec)
mysql&gt; quit
Bye</span></pre>
<p>Note that as this table is using the ndb (MySQL Cluster) storage engine, the data is actually held in the data nodes rather than in the SQL node and so we can access the exact same data from the the other SQL nodes:</p>
<pre style="padding-left: 30px; font-size: 11px"><span style="color: #993300;">[billy@ws1 7_0_6]$ bin/mysql -h 192.168.0.4 -P 3306
Welcome to the MySQL monitor.  Commands end with ; or g.
Your MySQL connection id is 10
Server version: 5.1.34-ndb-7.0.6-cluster-gpl-log MySQL Cluster Server (GPL)
Type 'help;' or 'h' for help. Type 'c' to clear the current input statement.
mysql&gt; use test;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A
Database changed
mysql&gt; select * from assets;
+------+-------+
| name | value |
+------+-------+
| Car  |  1900 |
+------+-------+
1 row in set (0.01 sec)
mysql&gt; quit
Bye
</span></pre>
<h3>Your next steps</h3>
<p>This is still a fairly simple, contrived set up. Hopefully it&#8217;s clear how additional data or SQL nodes could be added and in a larger deployment you may well decide to run the management and SQL nodes on different hosts to the data nodes (in fact, when starting up the management nodes there is a warning message suggesting you deploy them elsewhere!).</p>
<p>To move the management node to a 3rd, independent physical host (and a 4th one if you want 2 management nodes for redundancy -a future article will explain when you might want to do that),  just change the IP address in the  [ndb_mgmd] section of config.ini and then run the ndb_mgmd executable on that new host. Note that the management node consumes very few resources and so can share that host with other functions/applications (e.g. SQL Server nodes but not data nodes).</p>
<p>You’d also set several more variables in the configuration files in order to size and tune your Cluster.</p>
<p>In this article, I used 2 LINUX hosts but you could extend the Windows example introduced in <a href="./../../mysql-cluster/my-first-cluster-running-on-windows/index.html" target="_blank">My first Cluster running on Windows</a> in exactly the same way.</p>
]]></content:encoded>
					
					<wfw:commentRss>./../../mysql-cluster/deploying-mysql-cluster-over-multiple-hosts/feed/index.html</wfw:commentRss>
			<slash:comments>120</slash:comments>
		
		
			</item>
	</channel>
</rss>
