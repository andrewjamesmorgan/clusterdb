<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>MCM &#8211; Andrew Morgan on Databases</title>
	<atom:link href="/tag/mcm/feed" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>Database technologies - especially around scalability and High Availability</description>
	<lastBuildDate>Wed, 30 Apr 2014 12:02:52 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	
	<item>
		<title>MySQL Cluster Manager 1.3.1 released</title>
		<link>/mysql-cluster/mysql-cluster-manager-1-3-1-released</link>
					<comments>/mysql-cluster/mysql-cluster-manager-1-3-1-released#respond</comments>
		
		<dc:creator><![CDATA[andrew]]></dc:creator>
		<pubDate>Wed, 30 Apr 2014 12:02:52 +0000</pubDate>
				<category><![CDATA[MySQL Cluster]]></category>
		<category><![CDATA[MCM]]></category>
		<category><![CDATA[MySQL]]></category>
		<category><![CDATA[MySQL Cluster Manager]]></category>
		<guid isPermaLink="false">/?p=3884</guid>

					<description><![CDATA[MySQL Cluster Manager 1.3.1 is now available to download from My Oracle Support and soon from the Oracle Software Delivery Cloud. Details are available in the the MCM 1.3.1 Release Notes . Documentation is available here.]]></description>
										<content:encoded><![CDATA[<p><a href="/wp-content/uploads/2013/06/MySQL_Cluster_Manager2.png"><img decoding="async" src="/wp-content/uploads/2013/06/MySQL_Cluster_Manager2-300x106.png" alt="MySQL Cluster Manager logo" width="300" height="106" class="alignright size-medium wp-image-2846" /></a>MySQL Cluster Manager 1.3.1 is now available to <a href="https://support.oracle.com/" title="Download MySQL Cluster Manager" target="_blank">download from My Oracle Support</a> and soon from the <a href="https://edelivery.oracle.com/" title="Oracle Software Delivery Cloud" target="_blank">Oracle Software Delivery Cloud</a>.</p>
<p>Details are available in the the <a title="MySQL Cluster Manager - change history" href="http://dev.mysql.com/doc/relnotes/mysql-cluster-manager/1.3/en/mcm-news-1-3-1.html" target="_blank">MCM 1.3.1 Release Notes</a> .</p>
<p><a title="MySQL Cluster Manager documentation" href="http://dev.mysql.com/doc/relnotes/mysql-cluster-manager/1.3/en/index.html" target="_blank">Documentation is available here</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>/mysql-cluster/mysql-cluster-manager-1-3-1-released/feed</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>MySQL cluster management &#8211; webinar replay available</title>
		<link>/mysql-cluster/mysql-cluster-management-webinar-replay-available</link>
					<comments>/mysql-cluster/mysql-cluster-management-webinar-replay-available#respond</comments>
		
		<dc:creator><![CDATA[andrew]]></dc:creator>
		<pubDate>Mon, 10 Feb 2014 17:38:08 +0000</pubDate>
				<category><![CDATA[MySQL Cluster]]></category>
		<category><![CDATA[management]]></category>
		<category><![CDATA[MCM]]></category>
		<category><![CDATA[MySQL]]></category>
		<category><![CDATA[MySQL Cluster Manager]]></category>
		<guid isPermaLink="false">/?p=3539</guid>

					<description><![CDATA[Thomas Nielsen and I recently presented a webinar explaining the latest developments in managing MySQL Cluster. In case you weren&#8217;t able to attend (or wanted to refresh your memory) then the webinar replay and charts are now available. As a reminder, this webinar covered what&#8217;s new in MySQL Cluster Manager 1.3 which recently went GA.]]></description>
										<content:encoded><![CDATA[<p><a href="/wp-content/uploads/2014/01/Migrating-from-MySQL-Cluster-Auto-Installer-to-MCM.png"><img fetchpriority="high" decoding="async" src="/wp-content/uploads/2014/01/Migrating-from-MySQL-Cluster-Auto-Installer-to-MCM-197x300.png" alt="Migrating from MySQL Cluster Auto-Installer to MCM" width="197" height="300" class="alignright size-medium wp-image-3329" srcset="/wp-content/uploads/2014/01/Migrating-from-MySQL-Cluster-Auto-Installer-to-MCM-197x300.png 197w, /wp-content/uploads/2014/01/Migrating-from-MySQL-Cluster-Auto-Installer-to-MCM.png 317w" sizes="(max-width: 197px) 100vw, 197px" /></a><br />
Thomas Nielsen and I recently presented a webinar explaining the latest developments in managing MySQL Cluster. In case you weren&#8217;t able to attend (or wanted to refresh your memory) then the <a href="https://event.on24.com/eventRegistration/EventLobbyServlet?target=registration.jsp&#038;eventid=736173&#038;sessionid=1&#038;key=B9B7A14C3D7C094A249F40C919F2ACE4&#038;sourcepage=register" title="MySQL Cluster Manager webinar replay" target="blank">webinar replay and charts</a> are now available.</p>
<p><strong>As a reminder, this webinar covered what&#8217;s new in MySQL Cluster Manager 1.3 which recently went GA.</strong></p>
<p>By their very nature, clustered environments involve more efforts and resources to administer than standalone systems and this holds true for MySQL Cluster, the database designed for web-scale throughput with carrier-grade availability.</p>
<p>The MySQL Cluster Auto-Installer guides you through defining and running a well configured MySQL Cluster database &#8211; combining auto-discovery of platform resources with built-in best practices in an intuitive web-based GUI.</p>
<p>MySQL Cluster Manager (available as part of the commercial MySQL Cluster Carrier Grade Edition) simplifies the ongoing management of MySQL Cluster by automating common management tasks, delivering higher administration productivity and enhancing cluster agility. Tasks that used to take 46 commands can be reduced to just one! These tasks include configuration, starting &#038; stopping the cluster, upgrades and backup/restore <strong>and new for MCM 1.3, import a running Cluster</strong>.</p>
<p>These webinars are always a good opportunity to get your questions answered; here&#8217;s a catch up of the Q&#038;A from this session:</p>
<ul>
<li><strong>One of the biggest problems I am faced with is that many common applications heavily rely on the InnoDB or MyISAM &#8211; storage engines.So I am concerned about different behaviors of them compared to the NDB.</strong>This is something that we&#8217;ve been working to address &#8211; for example, JOINs in MySQL are now a <strong>lot</strong> faster and MySQL Cluster now supports Foreign Keys.</li>
<li><strong>What about transaction &#8211; Handling, Locking mechanisms, or even changes in supported statements which are related to instance &#8211; managment rather then being part of DDL, DML, or DQL?</strong> Yes, MySQL Cluster (NDB) is slightly different in these respects, mostly due to its real-time heritage and distributed nature.</li>
<li><strong>Is the MySQL &#8211; Cluster auto-installer restricted to specific operating system distributions or versions?</strong> No, the auto-installer works on all <a href="http://www.mysql.com/support/supportedplatforms/cluster.html" title="platforms supported by MySQL Cluster" target="blank">platforms supported by MySQL Cluster</a></li>
<li><strong> I hope there will be a white paper about the differences as this is the major reason why I am not changing to NDB right now after I changed from many &#8211; databases-in-one-instance to a one-database-per-instance approeach.</strong> There are a couple of resources available today: <a href="http://dev.mysql.com/doc/refman/5.6/en/mysql-cluster-ndb-innodb-engines.html" title="Reference Manual comparing InnoDB and MySQL Cluster" target="blank">Reference Manual comparing InnoDB and MySQL Cluster</a> and the <a href="http://www.mysql.com/why-mysql/white-papers/mysql-cluster-evaluation-guide/" title="MySQL Cluster Evaluation Guide" target="blank">MySQL Cluster Evaluation Guide</a></li>
<li><strong>In the creation of the site in MySQL Cluster Manager can hosts be specified using the IPv6 addresses or it&#8217;s IPv4-based only?</strong> At the moment, MCM only supports IPv4 addresses.</li>
<li><strong>Are performance criteria for the restoring of nodes in a cluster available?</strong> I&#8217;m not aware of performance benchmarks for a database restore but they&#8217;re fairly fast as they happen at the data node level (and so for example there&#8217;s no need to go through SQL). You can also <a href="http://dev.mysql.com/doc/refman/5.6/en/mysql-cluster-programs-ndb-restore.html#option_ndb_restore_parallelism" title="configure high degrees of parallelism for the restore of MySQL Cluster databases using the --parallelism option with MySQL Cluster" target="blank">configure high degrees of parallelism for the restore</a> so that many records are restored at once.</li>
</ul>
]]></content:encoded>
					
					<wfw:commentRss>/mysql-cluster/mysql-cluster-management-webinar-replay-available/feed</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>MySQL Cluster: The Latest Developments in Management, Free webinar</title>
		<link>/mysql-cluster/mysql-cluster-the-latest-developments-in-management-free-webinar</link>
					<comments>/mysql-cluster/mysql-cluster-the-latest-developments-in-management-free-webinar#comments</comments>
		
		<dc:creator><![CDATA[andrew]]></dc:creator>
		<pubDate>Thu, 16 Jan 2014 11:02:24 +0000</pubDate>
				<category><![CDATA[MySQL Cluster]]></category>
		<category><![CDATA[management]]></category>
		<category><![CDATA[MCM]]></category>
		<category><![CDATA[MySQL]]></category>
		<category><![CDATA[MySQL Cluster Manager]]></category>
		<guid isPermaLink="false">/?p=3327</guid>

					<description><![CDATA[On Thursday 23rd January, Thomas Nielsen and I will be hosting a webinar explaining the latest developments in managing MySQL Cluster. As always the webinar is free but please register here. Note that we&#8217;ll be covering what&#8217;s new in MySQL Cluster Manager 1.3 which went GA this week. By their very nature, clustered environments involve]]></description>
										<content:encoded><![CDATA[<p><a href="/wp-content/uploads/2014/01/Migrating-from-MySQL-Cluster-Auto-Installer-to-MCM.png"><img decoding="async" src="/wp-content/uploads/2014/01/Migrating-from-MySQL-Cluster-Auto-Installer-to-MCM-197x300.png" alt="Migrating from MySQL Cluster Auto-Installer to MCM" width="197" height="300" class="alignright size-medium wp-image-3329" srcset="/wp-content/uploads/2014/01/Migrating-from-MySQL-Cluster-Auto-Installer-to-MCM-197x300.png 197w, /wp-content/uploads/2014/01/Migrating-from-MySQL-Cluster-Auto-Installer-to-MCM.png 317w" sizes="(max-width: 197px) 100vw, 197px" /></a>On Thursday 23rd January, Thomas Nielsen and I will be hosting a webinar explaining the latest developments in managing MySQL Cluster. As always the webinar is free but please <a href="http://www.mysql.com/news-and-events/web-seminars/mysql-cluster-the-latest-developments-in-management/" title="register for MySQL Cluster management webinar" target="blank">register here</a>.</p>
<p><strong>Note that we&#8217;ll be covering what&#8217;s new in MySQL Cluster Manager 1.3 which went GA this week.</strong></p>
<p>By their very nature, clustered environments involve more efforts and resources to administer than standalone systems and this holds true for MySQL Cluster, the database designed for web-scale throughput with carrier-grade availability.</p>
<p>The MySQL Cluster Auto-Installer guides you through defining and running a well configured MySQL Cluster database &#8211; combining auto-discovery of platform resources with built-in best practices in an intuitive web-based GUI.</p>
<p>MySQL Cluster Manager (available as part of the commercial MySQL Cluster Carrier Grade Edition) simplifies the ongoing management of MySQL Cluster by automating common management tasks, delivering higher administration productivity and enhancing cluster agility. Tasks that used to take 46 commands can be reduced to just one! These tasks include configuration, starting &#038; stopping the cluster, upgrades and backup/restore <strong>and new for MCM 1.3, import a running Cluster</strong>.</p>
<p>Join this webcast to get up to speed on the latest developments in these tools and learn how to exploit them to make management of MySQL Cluster simple, efficient and reliable.</p>
<p>Times:</p>
<ul>
<li>Thu, Jan 23: 09:00 Pacific time (America)</li>
<li>Thu, Jan 23: 10:00 Mountain time (America)</li>
<li>Thu, Jan 23: 11:00 Central time (America)</li>
<li>Thu, Jan 23: 12:00 Eastern time (America)</li>
<li>Thu, Jan 23: 15:00 São Paulo time</li>
<li>Thu, Jan 23: 17:00 UTC</li>
<li>Thu, Jan 23: 17:00 Western European time</li>
<li>Thu, Jan 23: 18:00 Central European time</li>
<li>Thu, Jan 23: 19:00 Eastern European time</li>
<li>Thu, Jan 23: 22:30 India, Sri Lanka</li>
<li>Fri, Jan 24: 01:00 Singapore/Malaysia/Philippines time</li>
<li>Fri, Jan 24: 01:00 China time</li>
<li>Fri, Jan 24: 02:00 日本</li>
<li>Fri, Jan 24: 04:00 NSW, ACT, Victoria, Tasmania (Australia)</li>
</ul>
<p>Even if you can&#8217;t join the live webinar, it&#8217;s worth registering as you&#8217;ll be emailed a link to the replay as soon as it&#8217;s available.</p>
]]></content:encoded>
					
					<wfw:commentRss>/mysql-cluster/mysql-cluster-the-latest-developments-in-management-free-webinar/feed</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
		<item>
		<title>MCM 1.3 is GA &#8211; Importing a running Cluster into MySQL Cluster Manager</title>
		<link>/mysql-cluster/mcm-1-3-is-ga-importing-a-running-cluster-into-mysql-cluster-manager</link>
					<comments>/mysql-cluster/mcm-1-3-is-ga-importing-a-running-cluster-into-mysql-cluster-manager#respond</comments>
		
		<dc:creator><![CDATA[andrew]]></dc:creator>
		<pubDate>Tue, 14 Jan 2014 12:08:03 +0000</pubDate>
				<category><![CDATA[MySQL Cluster]]></category>
		<category><![CDATA[MCM]]></category>
		<category><![CDATA[MySQL]]></category>
		<category><![CDATA[MySQL Cluster Manager]]></category>
		<guid isPermaLink="false">/?p=3259</guid>

					<description><![CDATA[MySQL Cluster Manager 1.3.0 is now Generally Available and can be downloaded from the Oracle Software Delivery Cloud. The release contains a number of enhancements including performance improvements, handling larger clusters and of course bug fixes. The other big feature is that you can now import an existing, running MySQL Cluster instance into MCM without]]></description>
										<content:encoded><![CDATA[<p><a href="/wp-content/uploads/2014/01/MySQL_Cluster_Manager2.png"><img loading="lazy" decoding="async" src="/wp-content/uploads/2014/01/MySQL_Cluster_Manager2-300x106.png" alt="MySQL Cluster Manager logo" width="300" height="106" class="alignright size-medium wp-image-3294" srcset="/wp-content/uploads/2014/01/MySQL_Cluster_Manager2-300x106.png 300w, /wp-content/uploads/2014/01/MySQL_Cluster_Manager2-1024x365.png 1024w, /wp-content/uploads/2014/01/MySQL_Cluster_Manager2-900x320.png 900w, /wp-content/uploads/2014/01/MySQL_Cluster_Manager2.png 1181w" sizes="auto, (max-width: 300px) 100vw, 300px" /></a><br />
MySQL Cluster Manager 1.3.0 is now Generally Available and can be downloaded from <a href="https://edelivery.oracle.com/" title="Download MySQL Cluster from the Oracle Software Delivery Cloud" target="blank">the Oracle Software Delivery Cloud</a>. The release contains a number of enhancements including performance improvements, handling larger clusters and of course bug fixes. The other big feature is that you can now import an existing, running MySQL Cluster instance into MCM without having to stop it first &#8211; this is the topic for this post.</p>
<p>In the past, we had a nice browser-based tool (the <a href="/mysql-cluster/mysql-cluster-7-3-auto-installer" title="MySQL Clster Auto-Installer" target="blank">MySQL Clster Auto-Installer</a>) to get a well configured cluster up and running (tuned to your environment) and we also had <a href="http://www.mysql.com/products/cluster/mcm/" title="MySQL Cluster Manager" target="blank">MySQL Cluster Manager</a> to simplify the ongoing management of the cluster. Unfortunately, if you wanted to migrate the cluster you&#8217;d created with the auto-installer (or built by hand) then you first had to shut it down and then follow a manual procedure. MCM 1.3 introduces an <code>import</code> command that takes a running cluster and brings it under the control of MCM <strong>without</strong> having to stop the cluster (or suspend reads or writes). There are still some manual steps involved and the first half of this post will step through this process:</p>
<ul>
<li><a href="#specify">Specify Cluster topology in MCM</a></li>
<li><a href="#prepare">Prepare running cluster for import</a></li>
<li><a href="#pid">Ensure correct PID files in place</a></li>
<li><a href="#import">Import cluster into MCM</a></li>
</ul>
<p>Once the import has been completed, the post will then step though a number of <a href="#test">MCM tasks</a> to test that everything has gone to plan and also to give a reminder of how simple operations such as upgrades, backup/restore and adding new nodes is once you&#8217;re using MCM.</p>
<p><a id="specify"></a>&nbsp;</p>
<h3>Specify Cluster topology in MCM</h3>
<p>For this example, a cluster is used that&#8217;s been created using the auto-installer that was a part of MySQL Cluster 7.3.2 (the version is signifficant as from MySQL Cluster 7.3.3, the auto-installer creates <code>.conf</code>) files for the <code>mysqld</code> processes rather than specifying everything on the command-line &#8211; that simplifies the import process).</p>
<p>Before going any further, some data is added to the database so that I can later check that it&#8217;s not been lost:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
mysql@connect13a ~]$ mysql -h 127.0.0.1 -P3306 -u root
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 2
Server version: 5.5.34-ndb-7.2.14-cluster-gpl MySQL Cluster Community Server (GPL)

Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> CREATE DATABASE clusterdb;USE clusterdb;
Query OK, 1 row affected (0.06 sec)

mysql> CREATE TABLE simples (id INT AUTO_INCREMENT PRIMARY KEY, time TIMESTAMP) ENGINE=ndb;
Query OK, 0 rows affected (0.15 sec)

mysql> REPLACE INTO simples VALUES ();
Query OK, 1 row affected (0.00 sec)

mysql> REPLACE INTO simples VALUES ();
Query OK, 1 row affected (0.01 sec)

mysql> REPLACE INTO simples VALUES ();
Query OK, 1 row affected (0.00 sec)

mysql> REPLACE INTO simples VALUES ();
Query OK, 1 row affected (0.00 sec)

mysql> SELECT * FROM simples;
+----+---------------------+
| id | time                |
+----+---------------------+
|  3 | 2014-01-07 11:13:30 |
|  1 | 2014-01-07 11:13:28 |
|  2 | 2014-01-07 11:13:29 |
|  4 | 2014-01-07 11:13:30 |
+----+---------------------+
4 rows in set (0.00 sec)

</pre>
<p>The topology of the resulting cluster can be checked using the <code>show</code> command:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
mysql@connect13b ~]$ cluster_7_2_14/bin/ndb_mgm -e show
Connected to Management Server at: localhost:1186
Cluster Configuration
---------------------
[ndbd(NDB)]     2 node(s)
id=1    @192.168.56.103  (mysql-5.5.34 ndb-7.2.14, Nodegroup: 0, Master)
id=2    @192.168.56.104  (mysql-5.5.34 ndb-7.2.14, Nodegroup: 0)

[ndb_mgmd(MGM)] 2 node(s)
id=49   @192.168.56.101  (mysql-5.5.34 ndb-7.2.14)
id=52   @192.168.56.102  (mysql-5.5.34 ndb-7.2.14)

[mysqld(API)]   8 node(s)
id=50 (not connected, accepting connect from 192.168.56.101)
id=51 (not connected, accepting connect from 192.168.56.101)
id=53 (not connected, accepting connect from 192.168.56.102)
id=54 (not connected, accepting connect from 192.168.56.102)
id=55   @192.168.56.101  (mysql-5.5.34 ndb-7.2.14)
id=56   @192.168.56.101  (mysql-5.5.34 ndb-7.2.14)
id=57   @192.168.56.102  (mysql-5.5.34 ndb-7.2.14)
id=58   @192.168.56.102  (mysql-5.5.34 ndb-7.2.14)	
</pre>
<p>To define (but not create) the cluster in MCM, the following entities need to be defined:</p>
<ul>
<li><strong>site</strong>: the list of hosts that the cluster will run on</li>
<li><strong>package</strong>: location of the cluster binaries on each of the hosts in the site</li>
<li><strong>cluster</strong>: the cluster itself (the collection of nodes/processes that make up the cluster)</li>
</ul>
<p>The MCM daemon <code>mcmd</code> must first be started on each of the target hosts and the <code>mcm</code> client run on any host before creating each of these entities:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">

[mysql@connect13a ~]$ ./mcm1.3/mcm1.3.0/bin/mcmd&
MySQL Cluster Manager 1.3.0 (64bit) started
Connect to MySQL Cluster Manager by running "/home/mysql/mcm1.3/mcm1.3.0/bin/mcm" -a connect13a.localdomain:1862

[mysql@connect13b ~]$ ./mcm1.3/mcm1.3.0/bin/mcmd&
[1] 3068

[mysql@connect13b ~]$ MySQL Cluster Manager 1.3.0 (64bit) started
Connect to MySQL Cluster Manager by running "/home/mysql/mcm1.3/mcm1.3.0/bin/mcm" -a connect13b.localdomain:1862

[mysql@connect13c ~]$  ./mcm1.3/mcm1.3.0/bin/mcmd&
[1] 1922
[mysql@connect13c ~]$ MySQL Cluster Manager 1.3.0 (64bit) started
Connect to MySQL Cluster Manager by running "/home/mysql/mcm1.3/mcm1.3.0/bin/mcm" -a connect13c.localdomain:1862

[mysql@connect13d ~]$  ./mcm1.3/mcm1.3.0/bin/mcmd&
[1] 1936
[mysql@connect13d ~]$ MySQL Cluster Manager 1.3.0 (64bit) started
Connect to MySQL Cluster Manager by running "/home/mysql/mcm1.3/mcm1.3.0/bin/mcm" -a connect13d.localdomain:1862

[mysql@connect13a ~]$ ./mcm1.3/mcm1.3.0/bin/mcm
MySQL Cluster Manager client started.
This wrapper will spawn the mysql client to connect to mcmd

Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1
Server version: 1.3.0 MySQL Cluster Manager

Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mcm> CREATE SITE --hosts=192.168.56.101,192.168.56.102,192.168.56.103,192.168.56.104 mysite;
+---------------------------+
| Command result            |
+---------------------------+
| Site created successfully |
+---------------------------+
1 row in set (1.34 sec)

mcm> add package --basedir=/home/mysql/cluster_7_2_14 7_2_14;
+----------------------------+
| Command result             |
+----------------------------+
| Package added successfully |
+----------------------------+
1 row in set (0.40 sec)

mcm> CREATE CLUSTER --import --package=7_2_14 --processhosts=ndb_mgmd:49@192.168.56.101,
ndb_mgmd:52@192.168.56.102,ndbmtd:1@192.168.56.103,ndbmtd:2@192.168.56.104,mysqld:55@192.168.56.101,
mysqld:56@192.168.56.101,mysqld:57@192.168.56.102,mysqld:58@192.168.56.102,ndbapi:50@192.168.56.101,
ndbapi:51@192.168.56.101,ndbapi:53@192.168.56.102,ndbapi:54@192.168.56.102 mycluster;
+------------------------------+
| Command result               |
+------------------------------+
| Cluster created successfully |
+------------------------------+
1 row in set (0.34 sec)

mcm> SHOW STATUS -r mycluster;
+--------+----------+-----------------+--------+-----------+---------+
| NodeId | Process  | Host            | Status | Nodegroup | Package |
+--------+----------+-----------------+--------+-----------+---------+
| 49     | ndb_mgmd | 192.168.56.101  | import |           | 7_2_14  |
| 52     | ndb_mgmd | 192.168.56.102  | import |           | 7_2_14  |
| 1      | ndbmtd   | 192.168.56.103  | import | n/a       | 7_2_14  |
| 2      | ndbmtd   | 192.168.56.104  | import | n/a       | 7_2_14  |
| 55     | mysqld   | 192.168.56.101  | import |           | 7_2_14  |
| 56     | mysqld   | 192.168.56.101  | import |           | 7_2_14  |
| 57     | mysqld   | 192.168.56.102  | import |           | 7_2_14  |
| 58     | mysqld   | 192.168.56.102  | import |           | 7_2_14  |
| 50     | ndbapi   | *192.168.56.101 | import |           |         |
| 51     | ndbapi   | *192.168.56.101 | import |           |         |
| 53     | ndbapi   | *192.168.56.102 | import |           |         |
| 54     | ndbapi   | *192.168.56.102 | import |           |         |
+--------+----------+-----------------+--------+-----------+---------+
12 rows in set (0.06 sec)

</pre>
<p>This post doesn&#8217;t attempt to go into details about all of the commands shown above but if you&#8217;re new to MCM then check out the <a href="http://dev.mysql.com/doc/mysql-cluster-manager/1.3/en/index.html" title="MySQL Cluster documentation" target="blank">MySQL Cluster documentation</a>. The <code>--import</code> option sets the state of each of the nodes to <code>import</code>; they will stay in that state until the <code>import</code> command is run later.</p>
<p>When the auto-installer created the cluster, it changed a number of the configuration parameters from their defaults (either with command-line options or using configuration files). Using the LINUX <code>ps -ef</code> command you can identify all of these settings (either directly or by examining the configuration files that are referenced):</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
[mysql@connect13a ~]$ ps -ef | grep ndb_mgmd
mysql     2766     1  2 Jan07 ?        00:34:27 /home/mysql/cluster_7_2_14/bin/ndb_mgmd --initial 
--ndb-nodeid=49 --config-dir=/home/mysql/MySQL_Cluster/49/ 
--config-file=/home/mysql/MySQL_Cluster/49/config.ini

[mysql@connect13b ~]$ ps -ef | grep ndb_mgmd
mysql     2426     1  2 Jan07 ?        00:32:26 /home/mysql/cluster_7_2_14/bin/ndb_mgmd --initial 
--ndb-nodeid=52 --config-dir=/home/mysql/MySQL_Cluster/52/ 
--config-file=/home/mysql/MySQL_Cluster/52/config.ini

[mysql@connect13a ~]$ ps -ef | grep mysqld
mysql     3289     1  1 Jan07 ?        00:15:07 /home/mysql/cluster_7_2_14/bin/mysqld --no-defaults 
--datadir=/home/mysql/MySQL_Cluster/55/ --tmpdir=/home/mysql/MySQL_Cluster/55/tmp 
--basedir=/home/mysql/cluster_7_2_14/ --port=3306 --ndbcluster --ndb-nodeid=55 
--ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186, 
--socket=/home/mysql/MySQL_Cluster/55/mysql.socket
mysql     3392     1  1 Jan07 ?        00:15:08 /home/mysql/cluster_7_2_14/bin/mysqld --no-defaults 
--datadir=/home/mysql/MySQL_Cluster/56/ --tmpdir=/home/mysql/MySQL_Cluster/56/tmp 
--basedir=/home/mysql/cluster_7_2_14/ --port=3307 --ndbcluster --ndb-nodeid=56 
--ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186, 
--socket=/home/mysql/MySQL_Cluster/56/mysql.socket

[mysql@connect13b ~]$ ps -ef | grep mysqld
mysql     2884     1  1 Jan07 ?        00:14:41 /home/mysql/cluster_7_2_14/bin/mysqld --no-defaults 
--datadir=/home/mysql/MySQL_Cluster/57/ --tmpdir=/home/mysql/MySQL_Cluster/57/tmp 
--basedir=/home/mysql/cluster_7_2_14/ --port=3306 --ndbcluster --ndb-nodeid=57 
--ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186, 
--socket=/home/mysql/MySQL_Cluster/57/mysql.socket
mysql     2981     1  1 Jan07 ?        00:14:42 /home/mysql/cluster_7_2_14/bin/mysqld --no-defaults 
--datadir=/home/mysql/MySQL_Cluster/58/ --tmpdir=/home/mysql/MySQL_Cluster/58/tmp 
--basedir=/home/mysql/cluster_7_2_14/ --port=3307 --ndbcluster --ndb-nodeid=58 
--ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186, 
--socket=/home/mysql/MySQL_Cluster/58/mysql.socket

[mysql@connect13c ~]$ ps -ef | grep ndbmtd
mysql     1822     1 12 Jan07 ?        02:35:06 /home/mysql/cluster_7_2_14/bin/ndbmtd 
--ndb-nodeid=1 --ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186,
mysql     1823  1822 12 Jan07 ?        02:35:06 /home/mysql/cluster_7_2_14/bin/ndbmtd 
--ndb-nodeid=1 --ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186,

[mysql@connect13d ~]$ ps -ef | grep ndbmtd | grep nodeid=2
mysql     1835     1  0 Jan07 ?        00:00:52 /home/mysql/cluster_7_2_14/bin/ndbmtd 
--ndb-nodeid=2 --ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186,
mysql     1836  1835 12 Jan07 ?        02:30:46 /home/mysql/cluster_7_2_14/bin/ndbmtd 
--ndb-nodeid=2 --ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186,

[mysql@connect13a ~]$ cat /home/mysql/MySQL_Cluster/49/config.ini

# Configuration file for MyCluster
#

[NDB_MGMD DEFAULT]
Portnumber=1186

[NDB_MGMD]
NodeId=49
HostName=192.168.56.101
DataDir=/home/mysql/MySQL_Cluster/49/
Portnumber=1186

[NDB_MGMD]
NodeId=52
HostName=192.168.56.102
DataDir=/home/mysql/MySQL_Cluster/52/
Portnumber=1186

[TCP DEFAULT]
SendBufferMemory=4M
ReceiveBufferMemory=4M

[NDBD DEFAULT]
BackupMaxWriteSize=1M
BackupDataBufferSize=16M
BackupLogBufferSize=4M
BackupMemory=20M
BackupReportFrequency=10
MemReportFrequency=30
LogLevelStartup=15
LogLevelShutdown=15
LogLevelCheckpoint=8
LogLevelNodeRestart=15
DataMemory=58M
IndexMemory=9M
MaxNoOfTables=4096
MaxNoOfTriggers=3500
NoOfReplicas=2
StringMemory=25
DiskPageBufferMemory=64M
SharedGlobalMemory=20M
LongMessageBuffer=32M
MaxNoOfConcurrentTransactions=16384
BatchSizePerLocalScan=512
FragmentLogFileSize=64M
NoOfFragmentLogFiles=16
RedoBuffer=32M
MaxNoOfExecutionThreads=2
StopOnError=false
LockPagesInMainMemory=1
TimeBetweenEpochsTimeout=32000
TimeBetweenWatchdogCheckInitial=60000
TransactionInactiveTimeout=60000
HeartbeatIntervalDbDb=15000
HeartbeatIntervalDbApi=15000

[NDBD]
NodeId=1
HostName=192.168.56.103
DataDir=/home/mysql/MySQL_Cluster/1/

[NDBD]
NodeId=2
HostName=192.168.56.104
DataDir=/home/mysql/MySQL_Cluster/2/

[MYSQLD DEFAULT]

[MYSQLD]
NodeId=55
HostName=192.168.56.101

[MYSQLD]
NodeId=56
HostName=192.168.56.101

[MYSQLD]
NodeId=57
HostName=192.168.56.102

[MYSQLD]
NodeId=58
HostName=192.168.56.102

[API]
NodeId=50
HostName=192.168.56.101

[API]
NodeId=51
HostName=192.168.56.101

[API]
NodeId=53
HostName=192.168.56.102

[API]
NodeId=54
HostName=192.168.56.102

</pre>
<p>Now that we have a view of all of the configuration parameters, a subset of them need to be applied to the definition of the cluster in MCM (or else MCM will override them). Note that not all of the definitions need porting to MCM &#8211; for example <code>Portnumber=1186</code> as that is already the default value and <code>configdir</code> as MCM will use its own. So, based on the command-line options provided to the executables and the configuration files, the configuration parameters for the cluster defined in MCM are set:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
mcm> SET SendBufferMemory:ndbmtd+ndbmtd=4M,ReceiveBufferMemory:ndbmtd+ndbmtd=4M,
SendBufferMemory:ndbmtd+mysqld=4M,ReceiveBufferMemory:ndbmtd+mysqld=4M,BackupLogBufferSize:ndbmtd=4M,
BackupMemory:ndbmtd=20M,BackupReportFrequency:ndbmtd=10,MemReportFrequency:ndbmtd=30,
LogLevelStartup:ndbmtd=15,LogLevelShutdown:ndbmtd=15,LogLevelCheckpoint:ndbmtd=8,
LogLevelNodeRestart:ndbmtd=15,DataMemory:ndbmtd=58M,IndexMemory:ndbmtd=9M,MaxNoOfTables:ndbmtd=4096,
MaxNoOfTriggers:ndbmtd=3500,SharedGlobalMemory:ndbmtd=20M,LongMessageBuffer:ndbmtd=32M,
MaxNoOfConcurrentTransactions:ndbmtd=16384,BatchSizePerLocalScan:ndbmtd=512,
FragmentLogFileSize:ndbmtd=64M,StopOnError:ndbmtd=true,LockPagesInMainMemory:ndbmtd=1,
TimeBetweenEpochsTimeout:ndbmtd=32000,TimeBetweenWatchdogCheckInitial:ndbmtd=60000,
TransactionInactiveTimeout:ndbmtd=60000,HeartbeatIntervalDbDb:ndbmtd=15000,
HeartbeatIntervalDbApi:ndbmtd=15000,DataDir:ndbmtd:1=/home/mysql/MySQL_Cluster/1/,
DataDir:ndbmtd:2=/home/mysql/MySQL_Cluster/2/,DataDir:mysqld:55=/home/mysql/MySQL_Cluster/55/,
DataDir:mysqld:56=/home/mysql/MySQL_Cluster/56/,DataDir:mysqld:57=/home/mysql/MySQL_Cluster/57/,
DataDir:mysqld:58=/home/mysql/MySQL_Cluster/58/,tmpdir:mysqld:55=/home/mysql/MySQL_Cluster/55/tmp,
tmpdir:mysqld:56=/home/mysql/MySQL_Cluster/56/tmp,tmpdir:mysqld:57=/home/mysql/MySQL_Cluster/57/tmp,
tmpdir:mysqld:58=/home/mysql/MySQL_Cluster/58/tmp,
socket:mysqld:55=/home/mysql/MySQL_Cluster/55/mysql.socket,
socket:mysqld:56=/home/mysql/MySQL_Cluster/56/mysql.socket,
socket:mysqld:57=/home/mysql/MySQL_Cluster/57/mysql.socket,
socket:mysqld:58=/home/mysql/MySQL_Cluster/58/mysql.socket,port:mysqld:56=3307,
port:mysqld:58=3307 mycluster;

+-----------------------------------+
| Command result                    |
+-----------------------------------+
| Cluster reconfigured successfully |
+-----------------------------------+
1 row in set (0.44 sec)

</pre>
<p>Note that as MCM is not yet managing the running cluster, you can break this up into multiple <code>SET</code> commands as it doesn&#8217;t need to restart any processes.<br />
<a id="prepare"></a>&nbsp;</p>
<h3>Prepare running cluster for import</h3>
<p>As MCM takes over some functions from MySQL Cluster, we need to ensure that a couple of rules are imposed on the cluster so that there are no conflicts. </p>
<p>MCM is responsible for making sure that the management nodes are using the correct version of the configuration data and so we don&#8217;t want the management nodes holding onto older versions &#8211; this leads to the first rule, that the configuration cache should be disabled. This means restarting the management nodes with the <code>config-cache</code> parameter set to <code>FALSE</code>:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
mysql@connect13a ~]$ ps -ef | grep ndb_mgmd
mysql     2766     1  2 Jan07 ?        00:34:27 /home/mysql/cluster_7_2_14/bin/ndb_mgmd --initial 
--ndb-nodeid=49 --config-dir=/home/mysql/MySQL_Cluster/49/ 
--config-file=/home/mysql/MySQL_Cluster/49/config.ini
[mysql@connect13a ~]$ kill -9 2766
[mysql@connect13a ~]$  /home/mysql/cluster_7_2_14/bin/ndb_mgmd --initial --ndb-nodeid=49 
--config-dir=/home/mysql/MySQL_Cluster/49/ --config-file=/home/mysql/MySQL_Cluster/49/config.ini 
--config-cache=FALSE

[mysql@connect13b ~]$ ps -ef | grep ndb_mgmd
mysql     2426     1  2 Jan07 ?        00:32:26 /home/mysql/cluster_7_2_14/bin/ndb_mgmd --initial 
--ndb-nodeid=52 --config-dir=/home/mysql/MySQL_Cluster/52/ 
--config-file=/home/mysql/MySQL_Cluster/52/config.ini
[mysql@connect13b ~]$ kill -9 2426
[mysql@connect13b ~]$  /home/mysql/cluster_7_2_14/bin/ndb_mgmd --initial --ndb-nodeid=52 
--config-dir=/home/mysql/MySQL_Cluster/52/ --config-file=/home/mysql/MySQL_Cluster/52/config.ini 
--config-cache=FALSE

</pre>
<p>MCM is responsible for making sure that a node is restarted in the event of the process stopping (only happens if <code>StopOnError</code> is set to <code>FALSE</code>) and so the data nodes no longer need their angel processes (the first of the 2 <code>ndbmtd</code> processes we see for each data node). This means the second rule is that all of these angel processes must be killed:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
[mysql@connect13d ~]$ ps -ef | grep ndbmtd | grep nodeid=1
mysql     1822     1  0 Jan07 ?        00:00:52 /home/mysql/cluster_7_2_14/bin/ndbmtd --ndb-nodeid=1 
--ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186,
mysql     1823  1822 12 Jan07 ?        02:30:46 /home/mysql/cluster_7_2_14/bin/ndbmtd --ndb-nodeid=1 
--ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186,
[mysql@connect13d ~]$ kill -9 1822

[mysql@connect13d ~]$ ps -ef | grep ndbmtd | grep nodeid=2
mysql     1835     1  0 Jan07 ?        00:00:52 /home/mysql/cluster_7_2_14/bin/ndbmtd --ndb-nodeid=2 
--ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186,
mysql     1836  1835 12 Jan07 ?        02:30:46 /home/mysql/cluster_7_2_14/bin/ndbmtd --ndb-nodeid=2 
--ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186,
[mysql@connect13d ~]$ kill -9 1835

</pre>
<p>In order to manager the cluster, MCM needs to be able to connect to each of the MySQL Servers and so the <code>mcmd</code> user must be created on each of the MySQL Servers (unless you&#8217;re exploiting the ability to <a href="/mysql-cluster/sharing-user-credentials-between-mysql-servers-with-cluster" title="share user credentials between multiple MySQL Servers" target="blank">share user credentials between multiple MySQL Servers</a> in which case it only needs doing once):</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
[mysql@connect13a ~]$ mysql -h 127.0.0.1 -P3306 -u root
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 5
Server version: 5.5.34-ndb-7.2.14-cluster-gpl MySQL Cluster Community Server (GPL)

Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademarkshow  of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> CREATE USER 'mcmd'@'%' IDENTIFIED BY 'super';
Query OK, 0 rows affected (0.01 sec)

mysql> GRANT ALL PRIVILEGES ON *.* TO 'mcmd'@'127.0.0.1' IDENTIFIED BY 'super' WITH GRANT OPTION;
Query OK, 0 rows affected (0.01 sec)

[mysql@connect13a ~]$ mysql -h 127.0.0.1 -P3307 -u root
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 5
Server version: 5.5.34-ndb-7.2.14-cluster-gpl MySQL Cluster Community Server (GPL)

Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademarkshow  of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> CREATE USER 'mcmd'@'%' IDENTIFIED BY 'super';
Query OK, 0 rows affected (0.01 sec)

mysql> GRANT ALL PRIVILEGES ON *.* TO 'mcmd'@'127.0.0.1' IDENTIFIED BY 'super' WITH GRANT OPTION;
Query OK, 0 rows affected (0.01 sec)

[mysql@connect13b ~]$ mysql -h 127.0.0.1 -P3306 -u root
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 5
Server version: 5.5.34-ndb-7.2.14-cluster-gpl MySQL Cluster Community Server (GPL)

Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademarkshow  of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> CREATE USER 'mcmd'@'%' IDENTIFIED BY 'super';
Query OK, 0 rows affected (0.01 sec)

mysql> GRANT ALL PRIVILEGES ON *.* TO 'mcmd'@'127.0.0.1' IDENTIFIED BY 'super' WITH GRANT OPTION;
Query OK, 0 rows affected (0.01 sec)

[mysql@connect13b ~]$ mysql -h 127.0.0.1 -P3307 -u root
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 5
Server version: 5.5.34-ndb-7.2.14-cluster-gpl MySQL Cluster Community Server (GPL)

Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademarkshow  of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> CREATE USER 'mcmd'@'%' IDENTIFIED BY 'super';
Query OK, 0 rows affected (0.01 sec)

mysql> GRANT ALL PRIVILEGES ON *.* TO 'mcmd'@'127.0.0.1' IDENTIFIED BY 'super' WITH GRANT OPTION;
Query OK, 0 rows affected (0.01 sec)

</pre>
<p>As the running cluster was generated by a pre-MySQL Cluster 7.3.3 version of the auto-installer, all of the <code>mysqld</code> settings were specified as command-line options. The MCM <code>import</code> command is very restrictive about what command-line options are allowed and so most of the options need moving to <code>.cnf</code> files and the the <code>mysqld</code> processes need to be restarted to use those configuration files (note that the cluster is still available through this process but there is a rolling restart of the MySQL Servers and so applications may need to switch ones they&#8217;re connected to temporarily &#8211; if this isn&#8217;t already handled by load ballancing). These are the configuration files that were manually created and the commands to restart each MySQL Server:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
[mysql@connect13a ~]$ cat  /home/mysql/MySQL_Cluster/55.cnf
[mysqld]
tmpdir=/home/mysql/MySQL_Cluster/55/tmp
datadir=/home/mysql/MySQL_Cluster/55/
basedir=/home/mysql/cluster_7_2_14/
socket=/home/mysql/MySQL_Cluster/55/mysql.socket
port=3306
ndbcluster
ndb-nodeid=55
ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186

[mysql@connect13a ~]$ mysqladmin -h 127.0.0.1 -P 3306 -u root shutdown
[mysql@connect13a ~]$ /home/mysql/cluster_7_2_14/bin/mysqld 
--defaults-file=/home/mysql/MySQL_Cluster/55.cnf&

[mysql@connect13a ~]$ cat /home/mysql/MySQL_Cluster/56.cnf
[mysqld]
tmpdir=/home/mysql/MySQL_Cluster/56/tmp
datadir=/home/mysql/MySQL_Cluster/56/
basedir=/home/mysql/cluster_7_2_14/
socket=/home/mysql/MySQL_Cluster/56/mysql.socket
port=3307
ndbcluster
ndb-nodeid=56
ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186

[mysql@connect13a ~]$ mysqladmin -h 127.0.0.1 -P 3307 -u root shutdown
[mysql@connect13a ~]$ /home/mysql/cluster_7_2_14/bin/mysqld 
--defaults-file=/home/mysql/MySQL_Cluster/56.cnf&

[mysql@connect13b ~]$ cat /home/mysql/MySQL_Cluster/57.cnf
[mysqld]
tmpdir=/home/mysql/MySQL_Cluster/57/tmp
datadir=/home/mysql/MySQL_Cluster/57/
basedir=/home/mysql/cluster_7_2_14/
socket=/home/mysql/MySQL_Cluster/57/mysql.socket
port=3306
ndbcluster
ndb-nodeid=57
ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186

[mysql@connect13b ~]$ mysqladmin -h 127.0.0.1 -P 3306 -u root shutdown
[mysql@connect13b ~]$ /home/mysql/cluster_7_2_14/bin/mysqld 
--defaults-file=/home/mysql/MySQL_Cluster/57.cnf&

[mysql@connect13b ~]$ cat /home/mysql/MySQL_Cluster/58.cnf
[mysqld]
tmpdir=/home/mysql/MySQL_Cluster/58/tmp
datadir=/home/mysql/MySQL_Cluster/58/
basedir=/home/mysql/cluster_7_2_14/
socket=/home/mysql/MySQL_Cluster/58/mysql.socket
port=3307
ndbcluster
ndb-nodeid=58
ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186

[mysql@connect13b ~]$ mysqladmin -h 127.0.0.1 -P 3307 -u root shutdown
[mysql@connect13b ~]$ /home/mysql/cluster_7_2_14/bin/mysqld 
--defaults-file=/home/mysql/MySQL_Cluster/58.cnf&

</pre>
<p><a id="pid"></a>&nbsp;</p>
<h3>Ensure correct PID files in place</h3>
<p>MCM tracks each of the processes in the cluster using the process IDs held in <code>pid</code> files. For the data nodes, MCM will automatically fetch the process IDs from the <code>pid</code> files from the running cluster and so we need to make sure that they&#8217;re accurate (that they&#8217;re in the right place and contain the correct IDs):</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
[mysql@connect13c ~]$ ps -ef | grep ndbmtd
mysql     1823     1 12 Jan07 ?        02:35:06 /home/mysql/cluster_7_2_14/bin/ndbmtd 
--ndb-nodeid=1 --ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186,

[mysql@connect13c ~]$ echo 1823 > MySQL_Cluster/1/ndb_1.pid

[mysql@connect13d ~]$ ps -ef | grep ndbmtd
mysql     1836     1 12 Jan07 ?        02:34:03 /home/mysql/cluster_7_2_14/bin/ndbmtd 
--ndb-nodeid=2 --ndb-connectstring=192.168.56.101:1186,192.168.56.102:1186,

[mysql@connect13d ~]$ echo 1836 > MySQL_Cluster/2/ndb_2.pid
</pre>
<p>For MySQL Servers and management nodes, the <code>pid</code> files need be created within the MCM directory:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
[mysql@connect13a ~]$ ps -ef | grep mysqld
mysql    11145  1226  1 09:49 pts/1    00:00:02 /home/mysql/cluster_7_2_14/bin/mysqld 
--defaults-file=/home/mysql/MySQL_Cluster/55.cnf
mysql    11178  1226  1 09:50 pts/1    00:00:01 /home/mysql/cluster_7_2_14/bin/mysqld 
--defaults-file=/home/mysql/MySQL_Cluster/56.cnf

[mysql@connect13a ~]$ echo 11145 > mcm1.3/mcm_data/clusters/mycluster/pid/pid.55
[mysql@connect13a ~]$ echo 11178 > mcm1.3/mcm_data/clusters/mycluster/pid/pid.56

mysql@connect13b ~]$ ps -ef | grep mysqld
mysql     11247  8477  1 09:55 pts/1    00:00:03 /home/mysql/cluster_7_2_14/bin/mysqld 
--defaults-file=/home/mysql/MySQL_Cluster/57.cnf
mysql     11253  8477  0 09:57 pts/1    00:00:01 /home/mysql/cluster_7_2_14/bin/mysqld 
--defaults-file=/home/mysql/MySQL_Cluster/58.cnf

[mysql@connect13b ~]$ echo 11247 > mcm1.3/mcm_data/clusters/mycluster/pid/pid.57
[mysql@connect13b ~]$ echo 11253 > mcm1.3/mcm_data/clusters/mycluster/pid/pid.58

[mysql@connect13a pid]$ ps -ef | grep ndb_mgmd
mysql     8859     1  2 08:23 ?        00:00:23 /home/mysql/cluster_7_2_14/bin/ndb_mgmd --initial 
--ndb-nodeid=49 --config-dir=/home/mysql/MySQL_Cluster/49/ 
--config-file=/home/mysql/MySQL_Cluster/49/config.ini --config-cache=FALSE

[mysql@connect13b ~]$ echo 8859 > mcm1.3/mcm_data/clusters/mycluster/pid/pid.49

[mysql@connect13b pid]$ ps -ef | grep ndb_mgmd
mysql     7092     1  1 08:25 ?        00:00:17 /home/mysql/cluster_7_2_14/bin/ndb_mgmd --initial 
--ndb-nodeid=52 --config-dir=/home/mysql/MySQL_Cluster/52/ 
--config-file=/home/mysql/MySQL_Cluster/52/config.ini --config-cache=FALSE

[mysql@connect13b ~]$ echo 7092 > mcm1.3/mcm_data/clusters/mycluster/pid/pid.52

</pre>
<p>The good news is that if there are any mistakes in any of these files then the <code>import cluster --dryrun</code> command will fail with a useful error message and so you can come back to fix things up.<br />
<a id="import"></a>&nbsp;</p>
<h3>Import cluster into MCM</h3>
<p>OK &#8211; you&#8217;ve now done the hard bits which is the prep work ahead of the actual importing of the cluster into MCM control, now it&#8217;s MCM&#8217;s turn to automate the actual install.</p>
<p>Before the real import, we use MCM to perform a dry-run to make sure that all of the prep work has been completed succesfully:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
mcm> import cluster --dryrun mycluster;
+-------------------------------------------------+
| Command result                                  |
+-------------------------------------------------+
| Import checks passed. Cluster ready for import. |
+-------------------------------------------------+
1 row in set (0.80 sec)

</pre>
<p>All that&#8217;s left before running the final import is to take a backup (just in cas something should go very wrong):</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
mysql@connect13a ~]$ ndb_mgm
-- NDB Cluster -- Management Client --

ndb_mgm> start backup
Waiting for completed, this may take several minutes
Node 1: Backup 1 started from node 49
Node 1: Backup 1 started from node 49 completed
StartGCP: 35747 StopGCP: 35750
#Records: 2063 #LogRecords: 0
Data: 51472 bytes Log: 0 bytes

</pre>
<p>And now, finally the import itself can be run:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
mcm> import cluster mycluster;
+-------------------------------+
| Command result                |
+-------------------------------+
| Cluster imported successfully |
+-------------------------------+
1 row in set (2.98 sec)

</pre>
<p>After all of the prep work, that seems a bit of an anticlimax! As a first check, make sure that all of the nodes (processes) have been imported correctly:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
mcm> show status -r mycluster;
+--------+----------+-----------------+---------+-----------+---------+
| NodeId | Process  | Host            | Status  | Nodegroup | Package |
+--------+----------+-----------------+---------+-----------+---------+
| 49     | ndb_mgmd | 192.168.56.101  | running |           | 7_2_14  |
| 52     | ndb_mgmd | 192.168.56.102  | running |           | 7_2_14  |
| 1      | ndbmtd   | 192.168.56.103  | running | 0         | 7_2_14  |
| 2      | ndbmtd   | 192.168.56.104  | running | 0         | 7_2_14  |
| 55     | mysqld   | 192.168.56.101  | running |           | 7_2_14  |
| 56     | mysqld   | 192.168.56.101  | running |           | 7_2_14  |
| 57     | mysqld   | 192.168.56.102  | running |           | 7_2_14  |
| 58     | mysqld   | 192.168.56.102  | running |           | 7_2_14  |
| 50     | ndbapi   | *192.168.56.101 | added   |           |         |
| 51     | ndbapi   | *192.168.56.101 | added   |           |         |
| 53     | ndbapi   | *192.168.56.102 | added   |           |         |
| 54     | ndbapi   | *192.168.56.102 | added   |           |         |
+--------+----------+-----------------+---------+-----------+---------+
12 rows in set (0.10 sec)
</pre>
<p>As a second check, make sure that the data hasn&#8217;t been lost:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
[mysql@connect13a ~]$ mysql -h 127.0.0.1 -P 3307 -u root -e 'SELECT * FROM clusterdb.simples'
+----+---------------------+
| id | time                |
+----+---------------------+
|  3 | 2014-01-07 12:08:40 |
|  5 | 2014-01-07 12:08:41 |
|  1 | 2014-01-07 12:08:38 |
|  2 | 2014-01-07 12:08:39 |
|  4 | 2014-01-07 12:08:41 |
+----+---------------------+
</pre>
<p>So that&#8217;s it, the cluster is now safely under the control of MCM. The next section performs some more tests of MCM and at the same time illustrates how much simpler some of these management steps are now we have MCM to help.<br />
<a id="test"></a>&nbsp;</p>
<h3>Try out MCM on the imported cluster</h3>
<p>This final section shows how to exploit some of the MCM features on the cluster. </p>
<p>On-line backup and (especially) restore is very simple using MCM but before that can be done, we want to add extra <code>ndbapi</code> slots so that the restore command can be executed on the hosts running the data nodes &#8211; fortunately this is straight-forward to do:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
mcm> add process -R ndbapi:59@192.168.56.103,ndbapi:60@192.168.56.104 mycluster;

+----------------------------+
| Command result             |
+----------------------------+
| Process added successfully |
+----------------------------+
1 row in set (2 min 32.91 sec)

</pre>
<p>Note that this process took more than 2 minutes &#8211; the reason for that is that behind the scenes it performed a rolling restart of all of the existing nodes (processes) to make them aware of the new nodes. We can now confirm that these nodes have been added:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
mcm> show status -r mycluster;
+--------+----------+-----------------+---------+-----------+---------+
| NodeId | Process  | Host            | Status  | Nodegroup | Package |
+--------+----------+-----------------+---------+-----------+---------+
| 49     | ndb_mgmd | 192.168.56.101  | running |           | 7_2_14  |
| 52     | ndb_mgmd | 192.168.56.102  | running |           | 7_2_14  |
| 1      | ndbmtd   | 192.168.56.103  | running | 0         | 7_2_14  |
| 2      | ndbmtd   | 192.168.56.104  | running | 0         | 7_2_14  |
| 55     | mysqld   | 192.168.56.101  | running |           | 7_2_14  |
| 56     | mysqld   | 192.168.56.101  | running |           | 7_2_14  |
| 57     | mysqld   | 192.168.56.102  | running |           | 7_2_14  |
| 58     | mysqld   | 192.168.56.102  | running |           | 7_2_14  |
| 50     | ndbapi   | *192.168.56.101 | added   |           |         |
| 51     | ndbapi   | *192.168.56.101 | added   |           |         |
| 53     | ndbapi   | *192.168.56.102 | added   |           |         |
| 54     | ndbapi   | *192.168.56.102 | added   |           |         |
| 59     | ndbapi   | *192.168.56.103 | added   |           |         |
| 60     | ndbapi   | *192.168.56.104 | added   |           |         |
+--------+----------+-----------------+---------+-----------+---------+
14 rows in set (0.03 sec)

</pre>
<p>Taking a backup of the database is a single command:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
mcm> backup cluster mycluster;
+-------------------------------+
| Command result                |
+-------------------------------+
| Backup completed successfully |
+-------------------------------+
1 row in set (5.76 sec)

</pre>
<p>Before performing the restore, the test data can be removed from the database:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
[mysql@connect13a ~]$ mysql -h 127.0.0.1 -P 3307 -u root -e 'DELETE FROM clusterdb.simples'

</pre>
<p>To restore the database, check the available backups and then restore the most recent one (note as only the data has been deleted, the <code>-M</code> option is used to indicate that meta-data doesn&#8217;t need to be restored). The <code>-I</code> option is used to specify that the backup with Id of 2 should be used:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
mcm> list backups mycluster;
+----------+--------+----------------+---------------------+---------+
| BackupId | NodeId | Host           | Timestamp           | Comment |
+----------+--------+----------------+---------------------+---------+
| 1        | 1      | 192.168.56.103 | 2014-01-08 08:31:35 |         |
| 1        | 2      | 192.168.56.104 | 2014-01-08 08:31:33 |         |
| 2        | 1      | 192.168.56.103 | 2014-01-09 05:03:13 |         |
| 2        | 2      | 192.168.56.104 | 2014-01-09 05:03:12 |         |
+----------+--------+----------------+---------------------+---------+
4 rows in set (0.19 sec)

mcm> restore cluster -I 2 -M mycluster;
+--------------------------------+
| Command result                 |
+--------------------------------+
| Restore completed successfully |
+--------------------------------+
1 row in set (9.40 sec)

</pre>
<p>To confirm that the restore has indeed been successful, check that the test data is back in the database:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
[mysql@connect13a ~]$ mysql -h 127.0.0.1 -P 3307 -u root -e 'SELECT * FROM clusterdb.simples'
+----+---------------------+
| id | time                |
+----+---------------------+
|  3 | 2014-01-07 12:08:40 |
|  5 | 2014-01-07 12:08:41 |
|  1 | 2014-01-07 12:08:38 |
|  2 | 2014-01-07 12:08:39 |
|  4 | 2014-01-07 12:08:41 |
+----+---------------------+

</pre>
<p>As a final test, the cluster is upgraded to MySQL Cluster 7.3.3; all that&#8217;s needed is to define the package (telling MCM where to find the new binaries on the target hosts) and then perform the upgrade:</p>
<pre style="font: normal normal normal 9px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px; color: #000080;">
mcm> add package --basedir=/home/mysql/mcm1.3_cluster/cluster 7_3_3;
+----------------------------+
| Command result             |
+----------------------------+
| Package added successfully |
+----------------------------+
1 row in set (0.65 sec)

mcm> upgrade cluster --package=7_3_3 mycluster;
+-------------------------------+
| Command result                |
+-------------------------------+
| Cluster upgraded successfully |
+-------------------------------+
1 row in set (4 min 15.96 sec)

</pre>
<h3>Conclusion</h3>
<p>The most demanded feature for MySQL Cluster Manager was the ability to take an existing (running) cluster and bring it under the control of MCM (rather than having to create the cluster using MCM in the first place). MCM 1.3 delivers this. As you&#8217;ll have noticed, the migration process involves a non-trivial amount of prep work; the reason for this is that making configuration changes to cluster is fairly involved, with a lot of moving parts. Hopefully you&#8217;ll also have obderved that once the cluster is under the control of MCM, the management process becomes <strong>much</strong> simpler and less prone to user error.</p>
<p>It would be great to hear how people get on with MCM 1.3 in general and with the import in particular, please <a href="http://edelivery.oracle.com/" title="download the MySQL Cluster Manager software" target="blank">download the MySQL Cluster Manager software</a> and try it out.</p>
]]></content:encoded>
					
					<wfw:commentRss>/mysql-cluster/mcm-1-3-is-ga-importing-a-running-cluster-into-mysql-cluster-manager/feed</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>MySQL Cluster Manager 1.1.1 (GA) Available</title>
		<link>/mysql-cluster/mysql-cluster-manager-1-1-1-ga-available</link>
					<comments>/mysql-cluster/mysql-cluster-manager-1-1-1-ga-available#comments</comments>
		
		<dc:creator><![CDATA[andrew]]></dc:creator>
		<pubDate>Thu, 14 Jul 2011 16:00:29 +0000</pubDate>
				<category><![CDATA[MySQL Cluster]]></category>
		<category><![CDATA[MCM]]></category>
		<category><![CDATA[MySQL]]></category>
		<category><![CDATA[MySQL Cluster Manager]]></category>
		<guid isPermaLink="false">/?p=1886</guid>

					<description><![CDATA[The latest (GA) version of MySQL Cluster Manager is available through Oracle&#8217;s E-Delivery site. You can download the software and try it out for yourselves (just select &#8220;MySQL Database&#8221; as the product pack, select your platform, click &#8220;Go&#8221; and then scroll down to get the software). So what&#8217;s new in this version If you&#8217;ve looked]]></description>
										<content:encoded><![CDATA[<p><a href="/wp-content/uploads/2010/11/MySQL-Cluster-Manager.jpg"><img loading="lazy" decoding="async" class="alignright size-full wp-image-1352" title="MySQL Cluster Manager" src="/wp-content/uploads/2010/11/MySQL-Cluster-Manager.jpg" alt="" width="205" height="141" /></a>The latest (GA) version of MySQL Cluster Manager is <a href="http://edelivery.oracle.com/" target="_blank">available through Oracle&#8217;s E-Delivery site</a>. You can download the software and try it out for yourselves (just select &#8220;MySQL Database&#8221; as the product pack, select your platform, click &#8220;Go&#8221; and then scroll down to get the software).</p>
<h3>So what&#8217;s new in this version</h3>
<p>If you&#8217;ve looked at MCM in the past then the first thing that you&#8217;ll notice is that it&#8217;s now much simpler to get it up and running &#8211; in particular the configuration and running of the agent has now been reduced to just running a single executable (called &quot;mcmd&quot;). </p>
<p>The second change is that you can now stop the MCM agents from within the MCM CLI &#8211; for example &quot;stop agents mysite&quot; will safely stop all of the agents running on the hosts defined by &quot;mysite&quot;.</p>
<p>Those 2 changes make it much simpler for the novice user to get up and running quickly; for the more expert user, the most signifficant change is that MCM can now manage multiple clusters.</p>
<p>Obviously, there are a bunch of more minor changes as well as bug fixes.</p>
<h3>Refresher &#8211; So What is MySQL Cluster Manager?</h3>
<p>MySQL Cluster Manager provides the ability to control the entire cluster as a single entity, while also supporting very granular control down to individual processes within the cluster itself.  Administrators are able to create and delete entire clusters, and to start, stop and restart the cluster with a single command.  As a result, administrators no longer need to manually restart each data node in turn, in the correct sequence, or to create custom scripts to automate the process.</p>
<p>MySQL Cluster Manager automates on-line management operations, including the upgrade, downgrade and reconfiguration of running clusters as well as adding nodes on-line for dynamic, on-demand scalability, without interrupting applications or clients accessing the database.  Administrators no longer need to manually edit configuration files and distribute them to other cluster nodes, or to determine if rolling restarts are required. MySQL Cluster Manager handles all of these tasks, thereby enforcing best practices and making on-line operations significantly simpler, faster and less error-prone.</p>
<p>MySQL Cluster Manager is able to monitor cluster health at both an Operating System and per-process level by automatically polling each node in the cluster.  It can detect if a process or server host is alive, dead or has hung, allowing for faster problem detection, resolution and recovery.</p>
<p>To deliver 99.999% availability, MySQL Cluster has the capability to self-heal from failures by automatically restarting failed Data Nodes, without manual intervention.  MySQL Cluster Manager extends this functionality by also monitoring and automatically recovering SQL and Management Nodes.</p>
<h3>How is it Implemented?</h3>
<div id="attachment_1355" style="width: 310px" class="wp-caption alignright"><a href="/wp-content/uploads/2010/11/MySQL-Cluster-Manager-Architecture.jpg"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-1355" class="size-medium wp-image-1355" title="MySQL Cluster Manager Architecture" src="/wp-content/uploads/2010/11/MySQL-Cluster-Manager-Architecture-300x177.jpg" alt="" width="300" height="177" /></a><p id="caption-attachment-1355" class="wp-caption-text">MySQL Cluster Manager Architecture</p></div>
<p>MySQL Cluster Manager is implemented as a series of agent processes that co-operate with each other to manage the MySQL Cluster deployment; one agent running on each host machine that will be running a MySQL Cluster node (process). The administrator uses the regular mysql command to connect to any one of the agents using the port number of the agent (defaults to 1862 compared to the MySQL Server default of 3306).</p>
<h3>How is it Used?</h3>
<p>When using MySQL Cluster Manager to manage your MySQL Cluster deployment, the administrator no longer edits the configuration files (for example config.ini and my.cnf); instead, these files are created and maintained by the agents. In fact, if those files are manually edited, the changes will be overwritten by the configuration information which is held within the agents. Each agent stores all of the cluster configuration data, but it only creates the configuration files that are required for the nodes that are configured to run on that host.</p>
<p>Similarly when using MySQL Cluster Manager, management actions must not be performed by the administrator using the ndb_mgm command (which directly connects to the management node meaning that the agents themselves would not have visibility of any operations performed with it).</p>
<p>When using MySQL Cluster Manager, the &#8216;angel&#8217; processes are no longer needed (or created) for the data nodes, as it becomes the responsibility of the agents to detect the failure of the data nodes and recreate them as required. Additionally, the agents extend this functionality to include the management nodes and MySQL Server nodes.</p>
<h3>Installing, Configuring &amp; Running MySQL Cluster Manager</h3>
<p>On <strong>each</strong> host that will run Cluster nodes, install the MCM agent. To do this, just download the zip file from <a href="http://edelivery.oracle.com/" target="_new">Oracle E-Delivery</a> and then extract the contents into a convenient location:</p>
<pre style="padding-left: 30px;"><span style="color: #000080;">$ unzip V27167-01.zip
$ tar xf mysql-cluster-manager-1.1.1-linux-rhel5-x86-32bit.tar.gz
$ mv mysql-cluster-manager-1.1.1-linux-rhel5-x86-32bit ~/mcm</span></pre>
<p>Starting the agent is then trivial (remember to reapeat on each host though): </p>
<pre style="padding-left: 30px;"><span style="color: #000080;">$ cd ~/mcm
$ bin/mcmd&</span></pre>
<p>Next, some examples of how to use MCM.</p>
<h3>Example 1: Create a Cluster from Scratch</h3>
<p>The first step is to connect to one of the agents and then define the set of hosts that will be used for the Cluster:</p>
<pre style="font: normal normal normal 12px/18px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px;"><span style="color: #800000;"><span style="color: #000080;">$ mysql -h 192.168.0.10 -P 1862 -u admin -psuper --prompt='mcm&gt; '</span> 
mcm&gt; create site --hosts=192.168.0.10,192.168.0.11,192.168.0.12,192.168.0.13 mysite;</span></pre>
<p>Next step is to tell the agents where they can find the Cluster binaries that are going to be used, define what the Cluster will look like (which nodes/processes will run on which hosts) and then start the Cluster:</p>
<pre style="font: normal normal normal 12px/18px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px;"><span style="color: #800000;">mcm&gt; add package --basedir=/usr/local/mysql_6_3_27a 6.3.27a; 
mcm&gt; create cluster --package=6.3.26 --processhosts=ndb_mgmd@192.168.0.10,ndb_mgmd@192.168.0.11, 
  ndbd@192.168.0.12,ndbd@192.168.0.13,ndbd@192.168.0.12, ndbd@192.168.0.13,mysqld@192.168.0.10,
  mysqld@192.168.0.11 mycluster; 
mcm&gt; start cluster mycluster; </span></pre>
<h3>Example 2: On-Line upgrade of a Cluster</h3>
<p>A great example of how MySQL Cluster Manager can simplify management operations is upgrading the Cluster software. If performing the upgrade by hand then there are dozens of steps to run through which is time consuming, tedious and subject to human error (for example, restarting nodes in the wrong order could result in an outage). With MySQL Cluster Manager, it is reduced to two commands &#8211; define where to find the new version of the software and then perform the rolling, in-service upgrade:</p>
<pre style="font: normal normal normal 12px/18px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px;"><span style="color: #800000;"><span style="color: #800000;">mcm&gt; add package --basedir=/usr/local/mysql_7_1_8 7.1.8; 
mcm&gt; upgrade cluster --package=7.1.8 mycluster;</span></span></pre>
<p>Behind the scenes, each node will be halted and then restarted with the new version &#8211; ensuring that there is no loss of service.</p>
<h3>Example 3:  Automated On-Line Add-Node</h3>
<div id="attachment_1363" style="width: 310px" class="wp-caption alignright"><a href="/wp-content/uploads/2010/11/Automated-On-Line-Add-Node.jpg"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-1363" class="size-medium wp-image-1363" title="Automated On-Line Add-Node" src="/wp-content/uploads/2010/11/Automated-On-Line-Add-Node-300x211.jpg" alt="" width="300" height="211" /></a><p id="caption-attachment-1363" class="wp-caption-text">Automated On-Line Add-Node</p></div>
<p>Since MySQL Cluster 7.0 it has been possible to add new nodes to a Cluster while it is still in service; there are a number of steps involved and as with on-line upgrades if the administrator makes a mistake then it could lead to an outage.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>We&#8217;ll now look at how this is automated when using MySQL Cluster Manager; the first step is to add any new hosts (servers) to the site and indicate where those hosts can find the Cluster software: </p>
<pre style="font: normal normal normal 12px/18px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px;"><span style="color: #800000;">mcm&gt; add hosts --hosts=192.168.0.14,192.168.0.15 mysite; 
mcm&gt; add package --basedir=/usr/local/mysql_7_1_8 
  --hosts=192.168.0.14,192.168.0.15 7_1_8;</span></pre>
<p>The new nodes can then be added to the Cluster and then started up:</p>
<pre style="font: normal normal normal 12px/18px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px;"><span style="color: #800000;">mcm&gt; add process --processhosts=mysqld@192.168.0.10,mysqld@192.168.0.11,ndbd@192.168.0.14,
  ndbd@192.168.0.15,ndbd@192.168.0.14,ndbd@192.168.0.15 mycluster; 
mcm&gt; start process --added mycluster; </span></pre>
<p>The Cluster has now been extended but you need to perform a final step from any of the MySQL Servers to repartition the existing Cluster tables to use the new data nodes:</p>
<pre style="font: normal normal normal 12px/18px Consolas, Monaco, 'Courier New', Courier, monospace; padding-left: 30px;"><span style="color: #008000;">mysql&gt; ALTER ONLINE TABLE &lt;table-name&gt; REORGANIZE PARTITION; 
mysql&gt; OPTIMIZE TABLE &lt;table-name&gt;;</span></pre>
<h3>Where can I found out more?</h3>
<p>There is a lot of extra information to help you understand what can be achieved with MySQL Cluster Manager and how to use it:</p>
<ul>
<li><a href="http://www.mysql.com/products/cluster/mcm/cluster_install_demo.html" target="_blank">MySQL Cluster Manager Demo Video</a></li>
<li><a href="http://www.mysql.com/why-mysql/white-papers/mysql_wp_cluster_manager.php" target="_blank">MySQL Cluster Manager White Paper</a></li>
<li><a href="http://dev.mysql.com/doc/index-cluster.html" target="_blank">MySQL Cluster (including Manager) Documentation</a></li>
<li><a href="http://edelivery.oracle.com/" target="_blank">Download and try out MySQL Cluster Manager for yourself</a> &#8211; find it under &#8220;MySQL Database&#8221;</li>
</ul>
]]></content:encoded>
					
					<wfw:commentRss>/mysql-cluster/mysql-cluster-manager-1-1-1-ga-available/feed</wfw:commentRss>
			<slash:comments>19</slash:comments>
		
		
			</item>
	</channel>
</rss>
