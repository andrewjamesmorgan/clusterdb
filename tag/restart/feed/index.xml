<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>restart &#8211; Andrew Morgan on Databases</title>
	<atom:link href="/tag/restart/feed" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>Database technologies - especially around scalability and High Availability</description>
	<lastBuildDate>Thu, 12 Nov 2009 16:41:00 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	
	<item>
		<title>MySQL Cluster Restarts Get Faster</title>
		<link>/mysql-cluster/mysql-cluster-restarts-get-faster</link>
					<comments>/mysql-cluster/mysql-cluster-restarts-get-faster#comments</comments>
		
		<dc:creator><![CDATA[andrew]]></dc:creator>
		<pubDate>Thu, 12 Nov 2009 16:41:00 +0000</pubDate>
				<category><![CDATA[MySQL Cluster]]></category>
		<category><![CDATA[MySQL]]></category>
		<category><![CDATA[restart]]></category>
		<guid isPermaLink="false">/?p=664</guid>

					<description><![CDATA[MySQL Cluster 6.3.28b and 7.0.9b contain optmizations which can greatly reduce the time taken for data nodes to restart &#8211; this includes restarting a single node, performing a rolling restart or a full system restart. The benefits you see will depend on many factors including including the size of your database and the frequency, size]]></description>
										<content:encoded><![CDATA[<div class="mceTemp">MySQL Cluster 6.3.28b and 7.0.9b contain optmizations which can greatly reduce the time taken for data nodes to restart &#8211; this includes restarting a single node, performing a rolling restart or a full system restart.</div>
<p>The benefits you see will depend on many factors including including the size of your database and the frequency, size and complexity of your transactions. As an experiment, I re-ran some data node restart timings from an earlier post (<a href="/mysql-cluster/mysql-cluster-data-node-restart-times/">/mysql-cluster/mysql-cluster-data-node-restart-times/</a>)</p>
<p>The headline figure from my results is that for a 6 Gbyte database, with modest traffic I saw a <strong>2.2x</strong> improvement. This is using very simple transactions and so you may get a much better improvement &#8211; <strong>70x</strong> is being seen by some!!! The best news is that the slower your current restarts, the higher the benefit you can expect to see from the optimizations.</p>
<p>What follows is a reworking of that original post.</p>
<h3>MySQL Cluster Data Node restart times</h3>
<p>Restarts are required for certain, infrequent maintenance activities. Note that there is no loss of service while a single node restarts.</p>
<p>When a data node restarts, it first attempts to load the data into memory from the local log files and then it will catch up with any subsequent changes by retrieveing them from the surviving node(s) in its node group.</p>
<p>Based on this, you would expect the time taken to restart a data node to be influenced by:</p>
<ol>
<li>The amount of data that was stored on the data node before the restart</li>
<li>Rate of updates being made to the data during the restart</li>
<li>Network performance (assuming the data is being updated during recovery)</li>
</ol>
<p>The times will also be influenced bycertain configuration parameters, performance of the host machine and whether the multi-threaded data node (ndbmtd) is being used.</p>
<p>To provide some insight into how these factors impact restart times, tests have been performed where the following factors are varied:</p>
<ul>
<li>Database size (Each Gbyte is made up of 1,000,000 tuples in each of 5 tables)</li>
<li>Whether traffic is running or not (a single thread using the NDB API to send in up to 2K tps (10K updates/second))</li>
<li>Whether the 2 data nodes in the node group are on the same host or separated by a Gbit Ethernet network</li>
</ul>
<p>The following factors are kept constant:</p>
<ul>
<li>Physical hosts: Intel Core 2 Quad Q8200@2.33 GHz; 7.7 GBytes RAM</li>
<li>NoOfFragmentLogFiles: 300</li>
<li>MaxNoOfExecutionThreads=4</li>
</ul>
<p>The optimizations introduced in MySQL Cluster 6.3.28a and 7.0.9a have reduced these times &#8211; especially when write transactions are running before, during and after the node restart is triggered.</p>
<p>Here are the observed results:</p>
<div id="attachment_667" style="width: 632px" class="wp-caption alignnone"><a href="/wp-content/uploads/2009/11/improved_restart_times.jpg"><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-667" class="size-full wp-image-667" title="Improved Data Node Restart Times" src="/wp-content/uploads/2009/11/improved_restart_times.jpg" alt="Improved Data Node Restart Times" width="622" height="156" srcset="/wp-content/uploads/2009/11/improved_restart_times.jpg 622w, /wp-content/uploads/2009/11/improved_restart_times-300x75.jpg 300w" sizes="(max-width: 622px) 100vw, 622px" /></a><p id="caption-attachment-667" class="wp-caption-text">Improved Data Node Restart Times</p></div>
<p>For comparrison purposes, these are the results before the optimizations were introduced:</p>
<div id="attachment_369" style="width: 597px" class="wp-caption alignnone"><a href="/wp-content/uploads/2009/07/restart_times.jpg"><img decoding="async" aria-describedby="caption-attachment-369" class="size-full wp-image-369 " title="Old Data Node restart times" src="/wp-content/uploads/2009/07/restart_times.jpg" alt="Data Node restart times" width="587" height="168" srcset="/wp-content/uploads/2009/07/restart_times.jpg 587w, /wp-content/uploads/2009/07/restart_times-300x85.jpg 300w" sizes="(max-width: 587px) 100vw, 587px" /></a><p id="caption-attachment-369" class="wp-caption-text">Old Data Node restart times</p></div>
<p>There are a couple of things to note from these results:</p>
<ul>
<li>The optmizations greatly improve the results when update traffic is running</li>
<li>Using the multi-threaded data node (ndbmtd) greatly improves the restart time (in this case, 4 threads were available, improvements could be even greater on an 8 core/thread system)</li>
<li>Results become less predictable when heavy update traffic is being processed (in this case, up to 10,000 updated rows/second on a single node group). In the tests, no attempt was made to regulate this traffic and the test application was run on the same host as the one of the data nodes. Changes to the rate of updates will vary how long it takes for the restarting node to catch-up as it&#8217;s a moving target.</li>
</ul>
<p>There is another recovery/restart scenario. The measurements shown above assumed that the file system on the data node&#8217;s host was intact and could be used to recover the in-memory copy &#8211; if that were not the case (or the data nodes were restarted with the &#8220;initial&#8221; option) then all of the data would have to be recovered from the surviving data node(s) in the same node group. As a comparison restarting a 6 Gbyte data node with the &#8220;initial&#8221; option took 20 minutes compared to 8.5 minutes without it (ndbmtd, over Gbit n/w).</p>
]]></content:encoded>
					
					<wfw:commentRss>/mysql-cluster/mysql-cluster-restarts-get-faster/feed</wfw:commentRss>
			<slash:comments>1</slash:comments>
		
		
			</item>
		<item>
		<title>MySQL Cluster Data Node restart times</title>
		<link>/mysql-cluster/mysql-cluster-data-node-restart-times</link>
					<comments>/mysql-cluster/mysql-cluster-data-node-restart-times#comments</comments>
		
		<dc:creator><![CDATA[andrew]]></dc:creator>
		<pubDate>Thu, 30 Jul 2009 12:46:00 +0000</pubDate>
				<category><![CDATA[MySQL Cluster]]></category>
		<category><![CDATA[Cluster]]></category>
		<category><![CDATA[Cluster 7.0]]></category>
		<category><![CDATA[Cluster Database]]></category>
		<category><![CDATA[MySQL]]></category>
		<category><![CDATA[MySQL Cluster 7.0]]></category>
		<category><![CDATA[MySQL Cluster CGE]]></category>
		<category><![CDATA[recovery]]></category>
		<category><![CDATA[restart]]></category>
		<guid isPermaLink="false">/?p=365</guid>

					<description><![CDATA[Restart times have been reduced in MySQL Cluster 6.3.28a &#38; 7.0.9a &#8211; refer to that article for the new timings: /mysql-cluster/mysql-cluster-restarts-get-faster/ Restarts are required for certain, infrequent maintenance activities. Note that there is no loss of service while a single node restarts. When a data node restarts, it first attempts to load the data into memory]]></description>
										<content:encoded><![CDATA[<p><strong>Restart times have been reduced in MySQL Cluster 6.3.28a &amp; 7.0.9a &#8211; refer to that article for the new timings:</strong> <a href="/mysql-cluster/mysql-cluster-restarts-get-faster/" target="_self">/mysql-cluster/mysql-cluster-restarts-get-faster/</a></p>
<p>Restarts are required for certain, infrequent maintenance activities. Note that there is no loss of service while a single node restarts.</p>
<p>When a data node restarts, it first attempts to load the data into memory from the local log files and then it will catch up with any subsequent changes by retrieveing them from the surviving node(s) in its node group.</p>
<p> Based on this, you would expect the time taken to restart a data node to be influenced by:</p>
<ol>
<li>The amount of data that was stored on the data node before the restart</li>
<li>Rate of updates being made to the data during the restart</li>
<li>Network performance (assuming the data is being updated during recovery)</li>
</ol>
<p>The times will also be influenced bycertain configuration parameters, performance of the host machine and whether the multi-threaded data node (ndbmtd) is being used.</p>
<p>To provide some insight into how these factors impact restart times, tests have been performed where the following factors are varied:</p>
<ul>
<li>Database size (Each Gbyte is made up of 1,000,000 tuples in each of 5 tables)</li>
<li>Whether traffic is running or not (a single thread using the NDB API to send in up to 2K tps (10K updates/second))</li>
<li>Whether the 2 data nodes in the node group are on the same host or separated by a Gbit Ethernet network</li>
</ul>
<p>The following factors are kept constant:</p>
<ul>
<li>Physical hosts: Intel Core 2 Quad Q8200@2.33 GHz; 7.7 GBytes RAM</li>
<li>NoOfFragmentLogFiles: 300</li>
<li>MaxNoOfExecutionThreads=4</li>
</ul>
<p>Here are the observed results:</p>
<div id="attachment_369" style="width: 597px" class="wp-caption alignnone"><a href="/wp-content/uploads/2009/07/restart_times.jpg"><img decoding="async" aria-describedby="caption-attachment-369" class="size-full wp-image-369" title="Data Node restart times" src="/wp-content/uploads/2009/07/restart_times.jpg" alt="Data Node restart times" width="587" height="168" srcset="/wp-content/uploads/2009/07/restart_times.jpg 587w, /wp-content/uploads/2009/07/restart_times-300x85.jpg 300w" sizes="(max-width: 587px) 100vw, 587px" /></a><p id="caption-attachment-369" class="wp-caption-text">Data Node restart times</p></div>
<p>There are a couple of things to note from these results:</p>
<ul>
<li>Using the multi-threaded data node (ndbmtd) greatly improves the restart time (in this case, 4 threads were available, improvements could be even greater on an 8 core/thread system)</li>
<li>Results become less predictable when heavy update traffic is being processed (in this case, up to 10,000 updated rows/second on a single node group). In the tests, no attempt was made to regulate this traffic and the test application was run on the same host as the one of the data nodes. Changes to the rate of updates will vary how long it takes for the restarting node to catch-up as it&#8217;s a moving target.</li>
</ul>
<p>There is another recovery/restart scenario. The measurements shown above assumed that the file system on the data node&#8217;s host was intact and could be used to recover the in-memory copy &#8211; if that were not the case (or the data nodes were restarted with the &#8220;initial&#8221; option) then all of the data would have to be recovered from the surviving data node(s) in the same node group. As a comparison restarting a 6 Gbyte data node with the &#8220;initial&#8221; option took 20 minutes compared to 8 minutes without it (ndbmtd, over Gbit n/w).</p>
]]></content:encoded>
					
					<wfw:commentRss>/mysql-cluster/mysql-cluster-data-node-restart-times/feed</wfw:commentRss>
			<slash:comments>12</slash:comments>
		
		
			</item>
	</channel>
</rss>
