<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Kafka &#8211; Andrew Morgan on Databases</title>
	<atom:link href="./feed/index.html" rel="self" type="application/rss+xml" />
	<link>./../../index.html</link>
	<description>Database technologies - especially around scalability and High Availability</description>
	<lastBuildDate>Fri, 25 Nov 2016 08:38:19 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	
	<item>
		<title>Powering Microservices with MongoDB, Docker, Kubernetes &#038; Kafka</title>
		<link>./../../mongodb/powering-microservices-with-mongodb-docker-kubernetes-kafka/index.html</link>
					<comments>./../../mongodb/powering-microservices-with-mongodb-docker-kubernetes-kafka/index.html#respond</comments>
		
		<dc:creator><![CDATA[andrew]]></dc:creator>
		<pubDate>Wed, 16 Nov 2016 14:19:41 +0000</pubDate>
				<category><![CDATA[MongoDB]]></category>
		<category><![CDATA[Apache Kafka]]></category>
		<category><![CDATA[containers]]></category>
		<category><![CDATA[docker]]></category>
		<category><![CDATA[Kafka]]></category>
		<category><![CDATA[Kubernetes]]></category>
		<category><![CDATA[microservices]]></category>
		<category><![CDATA[mongodb]]></category>
		<guid isPermaLink="false">./../../index.html?p=4301</guid>

					<description><![CDATA[The slides and recording from my session at MongoDB Europe 2016, are now available. The presentation covers Microservices and some of the key technologies that enable them. Session Summary Organisations are building their applications around microservice architectures because of the flexibility, speed of delivery, and maintainability they deliver. Want to try out MongoDB on your]]></description>
										<content:encoded><![CDATA[<p><a href="./../../wp-content/uploads/2016/11/MongoDB-Europe173-2.jpg"><img fetchpriority="high" decoding="async" class="alignright wp-image-4302 size-medium" src="./../../wp-content/uploads/2016/11/MongoDB-Europe173-2-300x178.jpg" alt="Andrew Morgan presenting on Microservices at MongoDB Europe 2016" width="300" height="178" srcset="./../../wp-content/uploads/2016/11/MongoDB-Europe173-2-300x178.jpg 300w, ./../../wp-content/uploads/2016/11/MongoDB-Europe173-2-768x456.jpg 768w, ./../../wp-content/uploads/2016/11/MongoDB-Europe173-2-1024x607.jpg 1024w" sizes="(max-width: 300px) 100vw, 300px" /></a>The slides and recording from my session at <a href="https://www.mongodb.com/europe16">MongoDB Europe 2016</a>, are now available. The presentation covers Microservices and some of the key technologies that enable them.</p>
<h2>Session Summary</h2>
<p>Organisations are building their applications around microservice architectures because of the flexibility, speed of delivery, and maintainability they deliver.</p>
<p>Want to try out MongoDB on your laptop? Execute a single command and you have a lightweight, self-contained sandbox; another command removes all trace when you&#8217;re done. Need an identical copy of your application stack in multiple environments? Build your own container image and then your entire development, test, operations, and support teams can launch an identical clone environment.</p>
<p>Containers are revolutionising the entire software lifecycle: from the earliest technical experiments and proofs of concept through development, test, deployment, and support. Orchestration tools manage how multiple containers are created, upgraded and made highly available. Orchestration also controls how containers are connected to build sophisticated applications from multiple, microservice containers.</p>
<p>This session introduces you to technologies such as Docker, Kubernetes &amp; Kafka which are driving the microservices revolution. Learn about containers and orchestration – and most importantly how to exploit them for stateful services such as MongoDB.</p>
<h2>Recording</h2>
<p><iframe width="600" height="400" src="https://www.youtube.com/embed/F_BuRIAnvqE" frameborder="0" allowfullscreen></iframe></p>
<h2>Slides</h2>
<p><iframe style="border: 1px solid #CCC; border-width: 1px; margin-bottom: 5px; max-width: 100%;" src="//www.slideshare.net/slideshow/embed_code/key/qXZ4e3pTgbkDDp" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" allowfullscreen="allowfullscreen"> </iframe></p>
<div style="margin-bottom: 5px;"><strong> <a title="Powering Microservices with MongoDB, Docker, Kubernetes &amp; Kafka – MongoDB Europe 2016" href="//www.slideshare.net/andrewjamesmorgan/powering-microservices-with-mongodb-docker-kubernetes-kafka-mongodb-europe-2016" target="_blank">Powering Microservices with MongoDB, Docker, Kubernetes &amp; Kafka – MongoDB Europe 2016</a> </strong> from <strong><a href="//www.slideshare.net/andrewjamesmorgan" target="_blank">Andrew Morgan</a></strong></div>
]]></content:encoded>
					
					<wfw:commentRss>./../../mongodb/powering-microservices-with-mongodb-docker-kubernetes-kafka/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Webinar Replay (EMEA) – Data Streaming with Apache Kafka &#038; MongoDB</title>
		<link>./../../mongodb/webinar-replay-emea-data-streaming-with-apache-kafka-mongodb/index.html</link>
					<comments>./../../mongodb/webinar-replay-emea-data-streaming-with-apache-kafka-mongodb/index.html#respond</comments>
		
		<dc:creator><![CDATA[andrew]]></dc:creator>
		<pubDate>Thu, 10 Nov 2016 12:25:17 +0000</pubDate>
				<category><![CDATA[MongoDB]]></category>
		<category><![CDATA[Apache Kafka]]></category>
		<category><![CDATA[Kafka]]></category>
		<category><![CDATA[mongodb]]></category>
		<category><![CDATA[streaming]]></category>
		<category><![CDATA[webinar]]></category>
		<guid isPermaLink="false">./../../index.html?p=4288</guid>

					<description><![CDATA[The replay from the MongoDB/Apache Kafka webinar that I co-presented with David Tucker from Confluent earlier this week is now available: The replay is now available: Data Streaming with Apache Kafka &#38; MongoDB. Abstract A new generation of technologies is needed to consume and exploit today&#8217;s real time, fast moving data sources. Apache Kafka, originally]]></description>
										<content:encoded><![CDATA[<p>The replay from the MongoDB/Apache Kafka webinar that I co-presented with David Tucker from Confluent earlier this week is now available:</p>
<p>The replay is now available: <a href="https://www.mongodb.com/presentations/webinar-data-streaming-with-apache-kafka-and-mongodb-emea" title="Data Streaming with Apache Kafka &amp; MongoDB">Data Streaming with Apache Kafka &amp; MongoDB</a>.</p>
<h2>Abstract</h2>
<p>A new generation of technologies is needed to consume and exploit today&#8217;s real time, fast moving data sources. Apache Kafka, originally developed at LinkedIn, has emerged as one of these key new technologies.</p>
<p>This webinar explores the use-cases and architecture for Kafka, and how it integrates with MongoDB to build sophisticated data-driven applications that exploit new sources of data.</p>
<p>Watch the webinar to learn:</p>
<ul>
<li>What MongoDB is and where it&#8217;s used</li>
<li>What data streaming is and where it fits into modern data architectures</li>
<li>How Kafka works, what it delivers, and where it&#8217;s used</li>
<li>How to operationalize the Data Lake with MongoDB &amp; Kafka</li>
<li>How MongoDB integrates with Kafka – both as a producer and a consumer of event &#8211; data</li>
</ul>
<h2>Slides</h2>
<p><iframe loading="lazy" src="//www.slideshare.net/slideshow/embed_code/key/tuq8jPQErNg1HT" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe></p>
<div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/andrewjamesmorgan/data-streaming-with-apache-kafka-mongodb-emea" title="Data Streaming with Apache Kafka &amp; MongoDB - EMEA" target="_blank">Data Streaming with Apache Kafka &amp; MongoDB &#8211; EMEA</a> </strong> from <strong><a target="_blank" href="//www.slideshare.net/andrewjamesmorgan">Andrew Morgan</a></strong> </div>
]]></content:encoded>
					
					<wfw:commentRss>./../../mongodb/webinar-replay-emea-data-streaming-with-apache-kafka-mongodb/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Webinar Replay: Data Streaming with Apache Kafka &#038; MongoDB</title>
		<link>./../../mongodb/webinar-replay-data-streaming-with-apache-kafka-mongodb/index.html</link>
					<comments>./../../mongodb/webinar-replay-data-streaming-with-apache-kafka-mongodb/index.html#respond</comments>
		
		<dc:creator><![CDATA[andrew]]></dc:creator>
		<pubDate>Tue, 20 Sep 2016 14:54:35 +0000</pubDate>
				<category><![CDATA[MongoDB]]></category>
		<category><![CDATA[Apache Kafka]]></category>
		<category><![CDATA[Kafka]]></category>
		<category><![CDATA[mongodb]]></category>
		<category><![CDATA[streaming]]></category>
		<category><![CDATA[webinar]]></category>
		<guid isPermaLink="false">./../../index.html?p=4259</guid>

					<description><![CDATA[I recently co-presented a webinar with David Tucker from Confluent. The replay is now available: Data Streaming with Apache Kafka &#38; MongoDB. Abstract A new generation of technologies is needed to consume and exploit today&#8217;s real time, fast moving data sources. Apache Kafka, originally developed at LinkedIn, has emerged as one of these key new]]></description>
										<content:encoded><![CDATA[<p>I recently co-presented a webinar with David Tucker from Confluent.</p>
<p>The replay is now available: <a href="https://www.mongodb.com/presentations/webinar-data-streaming-with-apache-kafka-and-mongodb" title="Data Streaming with Apache Kafka &amp; MongoDB">Data Streaming with Apache Kafka &amp; MongoDB</a>.</p>
<h2>Abstract</h2>
<p>A new generation of technologies is needed to consume and exploit today&#8217;s real time, fast moving data sources. Apache Kafka, originally developed at LinkedIn, has emerged as one of these key new technologies.</p>
<p>This webinar explores the use-cases and architecture for Kafka, and how it integrates with MongoDB to build sophisticated data-driven applications that exploit new sources of data.</p>
<p>Watch the webinar to learn:</p>
<ul>
<li>What MongoDB is and where it&#8217;s used</li>
<li>What data streaming is and where it fits into modern data architectures</li>
<li>How Kafka works, what it delivers, and where it&#8217;s used</li>
<li>How to operationalize the Data Lake with MongoDB &amp; Kafka<br />
How MongoDB integrates with Kafka – both as a producer and a consumer of event &#8211; data</li>
</ul>
<h2>Slides</h2>
<p><iframe loading="lazy" src="//www.slideshare.net/slideshow/embed_code/key/rjSE2U2ziN7cHe" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe></p>
<div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/mongodb/webinar-data-streaming-with-apache-kafka-mongodb" title="Webinar: Data Streaming with Apache Kafka &amp; MongoDB" target="_blank">Webinar: Data Streaming with Apache Kafka &amp; MongoDB</a> </strong> from <strong><a href="//www.slideshare.net/mongodb" target="_blank">MongoDB</a></strong> </div>
]]></content:encoded>
					
					<wfw:commentRss>./../../mongodb/webinar-replay-data-streaming-with-apache-kafka-mongodb/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>MongoDB &#038; Data Streaming – Implementing a MongoDB Kafka Consumer</title>
		<link>./../../mongodb/mongodb-data-streaming-implementing-a-mongodb-kafka-consumer/index.html</link>
					<comments>./../../mongodb/mongodb-data-streaming-implementing-a-mongodb-kafka-consumer/index.html#respond</comments>
		
		<dc:creator><![CDATA[andrew]]></dc:creator>
		<pubDate>Fri, 16 Sep 2016 08:27:25 +0000</pubDate>
				<category><![CDATA[MongoDB]]></category>
		<category><![CDATA[Apache Kafka]]></category>
		<category><![CDATA[data streaming]]></category>
		<category><![CDATA[Kafka]]></category>
		<category><![CDATA[mongodb]]></category>
		<guid isPermaLink="false">./../../index.html?p=4230</guid>

					<description><![CDATA[Data Streaming In today&#8217;s data landscape, no single system can provide all of the required perspectives to deliver real insight. Deriving the full meaning from data requires mixing huge volumes of information from many sources. At the same time, we&#8217;re impatient to get answers instantly; if the time to insight exceeds 10s of milliseconds then]]></description>
										<content:encoded><![CDATA[<h1>Data Streaming</h1>
<p>In today&#8217;s data landscape, no single system can provide all of the required perspectives to deliver real insight. Deriving the full meaning from data requires mixing huge volumes of information from many sources.</p>
<p>At the same time, we&#8217;re impatient to get answers instantly; if the time to insight exceeds 10s of milliseconds then the value is lost – applications such as high frequency trading, fraud detection, and recommendation engines can&#8217;t afford to wait. This often means analyzing the inflow of data before it even makes it to the database of record. Add in zero tolerance for data loss and the challenge gets even more daunting.</p>
<p>Kafka and data streams are focused on ingesting the massive flow of data from multiple fire-hoses and then routing it to the systems that need it – filtering, aggregating, and analyzing en-route.</p>
<p>This blog introduces Apache Kafka and then illustrates how to use MongoDB as a source (producer) and destination (consumer) for the streamed data. A more complete study of this topic can be found in the <a href="https://www.mongodb.com/collateral/data-streaming-with-apache-kafka-and-mongodb" title="Data Streaming with Kafka &amp; MongoDB">Data Streaming with Kafka &amp; MongoDB</a> white paper.</p>
<h1>Apache Kafka</h1>
<p>Kafka provides a flexible, scalable, and reliable method to communicate streams of event data from one or more <strong>producers</strong> to one or more <strong>consumers</strong>. Examples of <strong>events</strong> include:</p>
<ul>
<li>A periodic sensor reading such as the current temperature</li>
<li>A user adding an item to the shopping cart in an online store</li>
<li>A Tweet being sent with a specific hashtag</li>
</ul>
<p>Streams of Kafka events are organized into <strong>topics</strong>. A producer chooses a topic to send a given event to, and consumers select which topics they pull events from. For example, a financial application could pull NYSE stock trades from one topic, and company financial announcements from another in order to look for trading opportunities.</p>
<p>In Kafka, topics are further divided into <strong>partitions</strong> to support scale out. Each Kafka node (<strong>broker</strong>) is responsible for receiving, storing, and passing on all of the events from one or more partitions for a given topic. In this way, the processing and storage for a topic can be linearly scaled across many brokers. Similarly, an application may scale out by using many consumers for a given topic, with each pulling events from a discrete set of partitions.</p>
<div id="attachment_4231" style="width: 490px" class="wp-caption aligncenter"><a href="./../../wp-content/uploads/2016/09/Kafka_topic.png"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-4231" src="./../../wp-content/uploads/2016/09/Kafka_topic-1024x720.png" alt="Kafka Producers, Consumers, Topics, and Partitions" width="480" height="338" class="size-large wp-image-4231" srcset="./../../wp-content/uploads/2016/09/Kafka_topic-1024x720.png 1024w, ./../../wp-content/uploads/2016/09/Kafka_topic-300x211.png 300w, ./../../wp-content/uploads/2016/09/Kafka_topic-768x540.png 768w, ./../../wp-content/uploads/2016/09/Kafka_topic.png 1464w" sizes="auto, (max-width: 480px) 100vw, 480px" /></a><p id="caption-attachment-4231" class="wp-caption-text">Figure 1: Kafka Producers, Consumers, Topics, and Partitions</p></div>
<h1>MongoDB As A Kafka Consumer – A Java Example</h1>
<p>In order to use MongoDB as a Kafka consumer, the received events must be converted into BSON documents before they are stored in the database. In this example, the events are strings representing JSON documents. The strings are converted to Java objects so that they are easy for Java developers to work with; those objects are then transformed into BSON documents.</p>
<p>Complete source code, Maven configuration, and test data can be found further down, but here are some of the highlights; starting with the main loop for receiving and processing event messages from the Kafka topic:</p>
<p><script src="https://gist.github.com/am-MongoDB/c802bcd43fec731d219ee8ace58dd880.js"></script></p>
<p>The <code>Fish</code> class includes helper methods to hide how the objects are converted into BSON documents:</p>
<p><script src="https://gist.github.com/am-MongoDB/432ffc15d4f95209da70123c9d752826.js"></script></p>
<p>In a real application more would be done with the received messages – they could be combined with reference data read from MongoDB, acted on and then passed along the pipeline by publishing to additional topics. In this example, the final step is to confirm from the <code>mongo</code> shell that the data has been added to the database:</p>
<p><script src="https://gist.github.com/2fd1f4a7ecaf5dd6a426fc8b53e9b4c0.js"></script></p>
<h2>Full Java Code for MongoDB Kafka Consumer</h2>
<h3>Business Object – <code>Fish.java</code></h3>
<p><script src="https://gist.github.com/217f007071922dc2d7d37eed9fe7a789.js"></script></p>
<h3>Kafka Consumer for MongoDB – <code>MongoDBSimpleConsumer.java</code></h3>
<p>Note that this example consumer is written using the Kafka <em>Simple Consumer API</em> – there is also a Kafka <em>High Level Consumer API</em> which hides much of the complexity – including managing the offsets. The Simple API provides more control to the application but at the cost of writing extra code.<br />
<script src="https://gist.github.com/77eb68a6af50f93453a7c316dbe1c6b5.js"></script></p>
<h3>Maven Dependencies – <code>pom.xml</code></h3>
<p><script src="https://gist.github.com/03f9bfbbedfe4fa015854c9a39ba4bbc.js"></script></p>
<h3>Test Data – <code>Fish.json</code></h3>
<p>A sample of the test data injected into Kafka is shown below:</p>
<p><script src="https://gist.github.com/5e74c8913119107936f9a5594b7dd149.js"></script></p>
<p>For simple testing, this data can be injected into the <code>clusterdb-topic1</code> topic using the <code>kafka-console-producer.sh</code> command.</p>
<h1>Next Steps</h1>
<p>To learn much more about data streaming and how MongoDB fits in (including Apache Kafka and competing and complementary technologies) read the <a href="https://www.mongodb.com/collateral/data-streaming-with-apache-kafka-and-mongodb" title="Data Streaming with Kafka &amp; MongoDB">Data Streaming with Kafka &amp; MongoDB</a> white paper and watch the <a href="https://www.mongodb.com/presentations/webinar-data-streaming-with-apache-kafka-and-mongodb">webinar replay</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>./../../mongodb/mongodb-data-streaming-implementing-a-mongodb-kafka-consumer/feed/index.html</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
	</channel>
</rss>
